{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91021fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTER SUMMARY:\n",
      "  human: start=1435 - attn=103 - straight=8 - duration=1 = final=1323\n",
      "  twin : start=1435 - nonpaired_drop=112 = final=1323\n",
      "Q_indiv_mean: no observation dropped \n",
      "I_indiv_mean: no observation dropped \n",
      "Q_org_mean: no observation dropped \n",
      "I_org_mean: no observation dropped \n",
      "scenario_close_QI: no observation dropped \n",
      "scenario_open_QI: no observation dropped \n",
      "               study name persona specification      variable name  \\\n",
      "0  quantitative intuition       default persona       Q_indiv_mean   \n",
      "1  quantitative intuition       default persona       I_indiv_mean   \n",
      "2  quantitative intuition       default persona         Q_org_mean   \n",
      "3  quantitative intuition       default persona         I_org_mean   \n",
      "4  quantitative intuition       default persona  scenario_close_QI   \n",
      "5  quantitative intuition       default persona   scenario_open_QI   \n",
      "\n",
      "   correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                           0.513884              0.473092   \n",
      "1                                           0.480731              0.438190   \n",
      "2                                           0.251782              0.200611   \n",
      "3                                           0.196304              0.143933   \n",
      "4                                           0.033274             -0.020657   \n",
      "5                                          -0.034700             -0.088428   \n",
      "\n",
      "   CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0  0.552477                                          20.636216        \n",
      "1  0.521124                                          19.035496        \n",
      "2  0.301584                                           9.348714        \n",
      "3  0.247579                                           7.225865        \n",
      "4  0.087012                                           1.209360        \n",
      "5  0.019230                                          -1.261211        \n",
      "\n",
      "   accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                 0.920887    3.579281   3.573486  ...   \n",
      "1                                 0.903956    3.478020   3.713729  ...   \n",
      "2                                 0.818540    3.580823   3.014145  ...   \n",
      "3                                 0.889981    3.219199   3.479214  ...   \n",
      "4                                 0.594860    0.594104   0.999244  ...   \n",
      "5                                 0.712774    0.715797   0.996977  ...   \n",
      "\n",
      "   effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                         NaN                        NaN               0   \n",
      "1                         NaN                        NaN               0   \n",
      "2                         NaN                        NaN               0   \n",
      "3                         NaN                        NaN               0   \n",
      "4                         NaN                        NaN               0   \n",
      "5                         NaN                        NaN               0   \n",
      "\n",
      "   domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                  1                             0                    0   \n",
      "1                  1                             0                    0   \n",
      "2                  1                             0                    0   \n",
      "3                  1                             0                    0   \n",
      "4                  1                             0                    0   \n",
      "5                  1                             0                    0   \n",
      "\n",
      "   stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                   0                    0                    0         1323  \n",
      "1                   0                    0                    0         1323  \n",
      "2                   0                    0                    0         1323  \n",
      "3                   0                    0                    0         1323  \n",
      "4                   0                    0                    0         1323  \n",
      "5                   0                    0                    0         1323  \n",
      "\n",
      "[6 rows x 26 columns]\n",
      "   TWIN_ID variable_name     value respondent_type              study_name  \\\n",
      "0        1  Q_indiv_mean  3.722222           human  quantitative intuition   \n",
      "1        2  Q_indiv_mean  2.666667           human  quantitative intuition   \n",
      "2        3  Q_indiv_mean  3.555556           human  quantitative intuition   \n",
      "3        4  Q_indiv_mean  4.166667           human  quantitative intuition   \n",
      "4        5  Q_indiv_mean  3.388889           human  quantitative intuition   \n",
      "\n",
      "  specification_name  \n",
      "0    default persona  \n",
      "1    default persona  \n",
      "2    default persona  \n",
      "3    default persona  \n",
      "4    default persona  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#######CURRENTLY USING DAN'S OPENAI API KEY. REPLACE WITH APPROPRIATE KEY.\n",
    "# import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from openai import OpenAI\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.formatting.rule import CellIsRule\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# from scipy.stats import spearmanr, norm\n",
    "from openpyxl.utils import get_column_letter\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "# Load data\n",
    "study_name = \"quantitative intuition\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data labels anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data labels anonymized.csv\"\n",
    "# df_human = pd.read_csv(human_file, header=0,skiprows=[1,2])\n",
    "# df_twin  = pd.read_csv(twin_file,  header=0,skiprows=[1,2])\n",
    "\n",
    "\n",
    "##############################from Daniel:\n",
    "\n",
    "HUMAN_FILE = \"quantitative intuition human data labels anonymized.csv\"\n",
    "TWIN_FILE = \"quantitative intuition twins data labels anonymized.csv\"\n",
    "\n",
    "Q_INDIV_COLS = [f\"Q20_{i}\" for i in range(1, 20) if i != 9]  # drop Q20_9\n",
    "I_INDIV_COLS = [f\"Q1_{i}\" for i in range(1, 20)]\n",
    "Q_ORG_COLS = [f\"Q22_{i}\" for i in range(1, 8)]\n",
    "I_ORG_COLS = [f\"Q21_{i}\" for i in range(1, 10)]  # Q21_10 later excluded\n",
    "ATTN_CHECKS_REQUIRED = {\"Q20_9\": \"Somewhat disagree\", \"Q32\": \"-3\"}  # humans\n",
    "ATTN_CHECKS_SKIPPED = [\"Q21_10\"]\n",
    "\n",
    "STRAIGHT_LINE_COLS = [f\"Q20_{i}\" for i in range(13, 20)]\n",
    "DURATION_COL = \"Duration (in seconds)\"\n",
    "MIN_DURATION = 150\n",
    "LIKERT_MAP = {\n",
    "    \"Strongly disagree\": 1,\n",
    "    \"Somewhat disagree\": 2,\n",
    "    \"Neither agree nor disagree\": 3,\n",
    "    \"Somewhat agree\": 4,\n",
    "    \"Strongly agree\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "def clean_tid(df):\n",
    "    df[\"_tid\"] = pd.to_numeric(df[\"TWIN_ID\"], errors=\"coerce\")\n",
    "    df = df[df[\"_tid\"].notnull()].copy()\n",
    "    df[\"TWIN_ID\"] = df[\"_tid\"].astype(\"Int64\")\n",
    "    return df.drop(columns=\"_tid\")\n",
    "\n",
    "\n",
    "def item_group(col):\n",
    "    if col.startswith(\"Q20_\"):\n",
    "        return \"Q_indiv\"\n",
    "    if col.startswith(\"Q1_\"):\n",
    "        return \"I_indiv\"\n",
    "    if col.startswith(\"Q22_\"):\n",
    "        return \"Q_org\"\n",
    "    if col.startswith(\"Q21_\"):\n",
    "        return \"I_org\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def load_qi(path: str, label: str, apply_filters: bool):\n",
    "    df = pd.read_csv(path, header=0, skiprows=[1, 2], low_memory=False)\n",
    "    df = clean_tid(df)\n",
    "    stats = {\n",
    "        \"dataset\": label,\n",
    "        \"initial_n\": len(df),\n",
    "        \"removed_attention\": 0,\n",
    "        \"removed_straight\": 0,\n",
    "        \"removed_duration\": 0,\n",
    "    }\n",
    "\n",
    "    if DURATION_COL in df.columns:\n",
    "        df[\"_duration\"] = pd.to_numeric(df[DURATION_COL], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"_duration\"] = np.nan\n",
    "\n",
    "    likert = list(\n",
    "        dict.fromkeys(\n",
    "            Q_INDIV_COLS\n",
    "            + I_INDIV_COLS\n",
    "            + Q_ORG_COLS\n",
    "            + I_ORG_COLS\n",
    "            + (list(ATTN_CHECKS_REQUIRED) if apply_filters else [])\n",
    "            + ATTN_CHECKS_SKIPPED\n",
    "        )\n",
    "    )\n",
    "    likert = [c for c in likert if c in df.columns]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        if likert:\n",
    "            df[likert] = df[likert].apply(lambda s: s.astype(str).str.strip()).replace(LIKERT_MAP)\n",
    "\n",
    "    if apply_filters:\n",
    "        before = len(df)\n",
    "        for col, val in ATTN_CHECKS_REQUIRED.items():\n",
    "            if col in df.columns:\n",
    "                expected = LIKERT_MAP.get(val, val)\n",
    "\n",
    "                if col == \"Q32\":\n",
    "                    col_str = df[col].astype(str).str.strip().str.strip('\"')\n",
    "                    df = df[col_str == str(expected)]\n",
    "                else:\n",
    "                    df = df[df[col].astype(str).str.strip() == str(expected)]\n",
    "\n",
    "        stats[\"removed_attention\"] = before - len(df)\n",
    "\n",
    "        before = len(df)\n",
    "        if all(c in df.columns for c in STRAIGHT_LINE_COLS):\n",
    "            df = df[df.apply(lambda r: pd.Series(r[STRAIGHT_LINE_COLS]).nunique() > 1, axis=1)]\n",
    "        stats[\"removed_straight\"] = before - len(df)\n",
    "\n",
    "        before = len(df)\n",
    "        if df[\"_duration\"].notna().any():\n",
    "            df = df[(df[\"_duration\"] >= MIN_DURATION) | df[\"_duration\"].isna()]\n",
    "        stats[\"removed_duration\"] = before - len(df)\n",
    "\n",
    "    df[\"Q_indiv_mean\"] = df[[c for c in Q_INDIV_COLS if c in df.columns]].mean(axis=1)\n",
    "    df[\"I_indiv_mean\"] = df[[c for c in I_INDIV_COLS if c in df.columns]].mean(axis=1)\n",
    "    df[\"Q_org_mean\"] = df[[c for c in Q_ORG_COLS if c in df.columns]].mean(axis=1)\n",
    "    df[\"I_org_mean\"] = df[[c for c in I_ORG_COLS if c in df.columns]].mean(axis=1)\n",
    "\n",
    "    stats[\"final_n\"] = len(df)\n",
    "    return df, stats\n",
    "\n",
    "\n",
    "df_human, stats_human = load_qi(HUMAN_FILE, \"human\", apply_filters=True)\n",
    "df_twin, stats_twin = load_qi(TWIN_FILE, \"twin\", apply_filters=False)\n",
    "\n",
    "before_pair = len(df_twin)\n",
    "df_twin = df_twin[df_twin[\"TWIN_ID\"].isin(df_human[\"TWIN_ID\"])]\n",
    "stats_twin.update(\n",
    "    {\n",
    "        \"dataset\": \"twin (paired)\",\n",
    "        \"initial_n\": before_pair,\n",
    "        \"removed_attention\": 0,\n",
    "        \"removed_straight\": 0,\n",
    "        \"removed_duration\": 0,\n",
    "        \"removed_nonpaired\": before_pair - len(df_twin),\n",
    "        \"final_n\": len(df_twin),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(\"FILTER SUMMARY:\")\n",
    "print(\n",
    "    f\"  human: start={stats_human['initial_n']} - attn={stats_human['removed_attention']} \"\n",
    "    f\"- straight={stats_human['removed_straight']} - duration={stats_human['removed_duration']} \"\n",
    "    f\"= final={stats_human['final_n']}\"\n",
    ")\n",
    "print(\n",
    "    f\"  twin : start={stats_twin['initial_n']} - nonpaired_drop={stats_twin['removed_nonpaired']} \"\n",
    "    f\"= final={stats_twin['final_n']}\"\n",
    ")\n",
    "\n",
    "##################\n",
    "# add DV related to close-ended scenario\n",
    "prefix = \"Break the problem into parts\"\n",
    "\n",
    "# for both data‐frames, create scenario_close_QI = 1 if the response starts with that prefix, else 0\n",
    "for df in (df_human, df_twin):\n",
    "    df[\"scenario_close_QI\"] = (\n",
    "        df[\"Q33\"]  # replace with your actual column name\n",
    "        .fillna(\"\")  # avoid errors on NaN\n",
    "        .astype(str)  # ensure it’s text\n",
    "        .str.startswith(prefix)  # boolean mask\n",
    "        .astype(int)  # convert True/False → 1/0\n",
    "    )\n",
    "\n",
    "\n",
    "# add DV from open-ended scenario:\n",
    "FREE_RESP_COL = \"Q34\"  # ← the column you want to classify\n",
    "CLASSIFICATION_MODEL = \"gpt-4o-mini\"\n",
    "MODEL = CLASSIFICATION_MODEL\n",
    "CACHE_FILE = f\"{FREE_RESP_COL}_{specification_name}_open_classification_cache.csv\"\n",
    "\n",
    "prompt_template = (\n",
    "    \"In a research study, a subject was shown quantitative facts about a company, including that \"\n",
    "    \"sales are down, call center complaints are up, unresolved complaints are up, and personnel turnover is up. \"\n",
    "    \"They were asked to explain what is going on with the company in up to 100 words.\\n\\n\"\n",
    "    \"Classify the SUBJECT ANSWER as either 'summary' (one that just restates or describes the facts) or 'synthesis' \"\n",
    "    \"(one that uses a fact to try to explain another, e.g., 'low sales are due to poor customer service', \"\n",
    "    \"or provides an alternative explanation altogether, e.g., 'low sales and high complaints are both probably because the product is bad'). \"\n",
    "    'Return JSON with keys classification (summary|synthesis) and confidence (0-100 integer). SUBJECT ANSWER:\\n\"{ans}\"'\n",
    ")\n",
    "\n",
    "OPENAI_API_KEY = \"\"  # <-- INSERT YOUR KEY (run once to build cache)\n",
    "\n",
    "\n",
    "def classify_scenario_open_QI():\n",
    "    # 1) check cache\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        return pd.read_csv(CACHE_FILE)\n",
    "\n",
    "    # 2) sanity check API key\n",
    "    if OPENAI_API_KEY.startswith(\"YOUR_\"):\n",
    "        print(\"API key not set; skipping scenario_open_QI classification.\")\n",
    "        return pd.DataFrame(columns=[\"TWIN_ID\", \"dataset\", \"scenario_open_QI\", \"confidence\"])\n",
    "\n",
    "    #    openai.api_key = OPENAI_API_KEY\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(f\"[DEBUG] using OpenAI key: {OPENAI_API_KEY[:5]}…{OPENAI_API_KEY[-5:]}\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    def _process(df, suffix, label):\n",
    "        col = f\"{FREE_RESP_COL}\"\n",
    "        if col not in df.columns:\n",
    "            return\n",
    "        subset = df[[\"TWIN_ID\", col]].dropna()\n",
    "        total = len(subset)\n",
    "        for i, (tid, ans) in enumerate(subset.itertuples(index=False), 1):\n",
    "            text = str(ans).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            prompt = prompt_template.format(ans=text.replace('\"', '\\\\\"'))\n",
    "\n",
    "            backoff = 1\n",
    "            for attempt in range(4):\n",
    "                try:\n",
    "                    resp = client.chat.completions.create(\n",
    "                        model=MODEL,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a careful research assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt},\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                    )\n",
    "                    reply = resp.choices[0].message.content\n",
    "                    m = re.search(r\"\\{.*\\}\", reply, re.DOTALL)\n",
    "                    data = json.loads(m.group(0)) if m else {}\n",
    "                    cls = data.get(\"classification\", \"\").lower()\n",
    "                    # map summary→0, synthesis→1\n",
    "                    bin_ = 1 if cls == \"synthesis\" else 0\n",
    "                    rows.append((tid, label, bin_, data.get(\"confidence\", None)))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == 3:\n",
    "                        rows.append((tid, label, None, None))\n",
    "                        print(f\"[{label}] ✖ permanent failure at {i}/{total} (TWIN_ID={tid}): {e}\")\n",
    "                    else:\n",
    "                        time.sleep(backoff)\n",
    "                        print(\n",
    "                            f\"[{label}]  attempt {attempt + 1} error at {i}/{total}: {e} — retrying\"\n",
    "                        )\n",
    "                        time.sleep(backoff)\n",
    "                        backoff *= 2\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"[{label}] {i}/{total} classified…\")\n",
    "\n",
    "        print(f\"[{label}] done ({total} items)\")\n",
    "\n",
    "    _process(df_human, \"human\", \"human\")\n",
    "    _process(df_twin, \"twin\", \"twin\")\n",
    "\n",
    "    out = pd.DataFrame(rows, columns=[\"TWIN_ID\", \"dataset\", \"scenario_open_QI\", \"confidence\"])\n",
    "    out.to_csv(CACHE_FILE, index=False)\n",
    "    print(f\"Wrote cache → {CACHE_FILE}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ── run it ────────────────────────────────────────────────────────────────────\n",
    "sc_df = classify_scenario_open_QI()\n",
    "\n",
    "# ── pivot & merge back ───────────────────────────────────────────────────────\n",
    "pivot = sc_df.pivot(index=\"TWIN_ID\", columns=\"dataset\", values=\"scenario_open_QI\")\n",
    "# pivot.columns might be e.g. Index(['human','twin'], name='dataset')\n",
    "\n",
    "# ── map directly into each dataframe ─────────────────────────────────────────\n",
    "# for human:\n",
    "if \"human\" in pivot.columns:\n",
    "    df_human[\"scenario_open_QI\"] = df_human[\"TWIN_ID\"].map(pivot[\"human\"])\n",
    "else:\n",
    "    # if no human‐labels were classified, fill with NaN\n",
    "    df_human[\"scenario_open_QI\"] = np.nan\n",
    "\n",
    "# for twin:\n",
    "if \"twin\" in pivot.columns:\n",
    "    df_twin[\"scenario_open_QI\"] = df_twin[\"TWIN_ID\"].map(pivot[\"twin\"])\n",
    "else:\n",
    "    df_twin[\"scenario_open_QI\"] = np.nan\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "out_file = f\"{study_name} {specification_name} human data processed.csv\"\n",
    "df_human.to_csv(out_file, index=False)\n",
    "out_file = f\"{study_name} {specification_name} twins data processed.csv\"\n",
    "df_twin.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "\n",
    "# raw responses:\n",
    "raw_vars = [\"\"]\n",
    "# raw_vars_min = []\n",
    "# raw_vars_max = []\n",
    "# #raw responses: domain=social?\n",
    "# raw_vars_social=[]\n",
    "# raw_vars_social_map = dict(zip(raw_vars, raw_vars_social))\n",
    "# #raw responses: domain=cognitive?\n",
    "# raw_vars_cognitive=[]\n",
    "# raw_vars_cognitive_map = dict(zip(raw_vars, raw_vars_cognitive))\n",
    "# #raw responses: replicating know human bias?\n",
    "# raw_vars_known=[]\n",
    "# raw_vars_known_map = dict(zip(raw_vars, raw_vars_known))\n",
    "# #raw responses: preference measure?\n",
    "# raw_vars_pref=[]\n",
    "# raw_vars_pref_map = dict(zip(raw_vars, raw_vars_pref))\n",
    "# #raw responses: stimuli dependent?\n",
    "# raw_vars_stim=[]\n",
    "# raw_vars_stim_map = dict(zip(raw_vars, raw_vars_stim))\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [\n",
    "    \"Q_indiv_mean\",\n",
    "    \"I_indiv_mean\",\n",
    "    \"Q_org_mean\",\n",
    "    \"I_org_mean\",\n",
    "    \"scenario_close_QI\",\n",
    "    \"scenario_open_QI\",\n",
    "]\n",
    "DV_vars_min = [1, 1, 1, 1, 0, 0]\n",
    "DV_vars_max = [5, 5, 5, 5, 1, 1]\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [0] * 6\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [1] * 6\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0] * 6\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [0] * 6\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [0] * 6\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [0] * 6\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0] * 6\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "# Fix dtypes\n",
    "for var in DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "# for var in raw_vars:\n",
    "#     ##############################\n",
    "#     #07/18/26: condition assignment different for each DV.\n",
    "# #     col_h = f\"{var}_human\"\n",
    "# #     col_t = f\"{var}_twin\"\n",
    "# #     min_col = f\"{var}_min\"\n",
    "# #     max_col = f\"{var}_max\"\n",
    "# #     if cond_exists:\n",
    "# #         cols = [col_h, col_t, cond_h, cond_t,min_col,max_col]\n",
    "# #     else:\n",
    "# #         cols = [col_h, col_t,min_col,max_col]\n",
    "# #     pair = (\n",
    "# #     df[cols]\n",
    "# #       .dropna(subset=[col_h, col_t])\n",
    "# #     )\n",
    "# # look up the right condition for this DV\n",
    "#     cond    = dv_to_cond[var]           # e.g. 'condition_green'\n",
    "#     cond_h  = f\"{cond}_human\"           # 'condition_green_human'\n",
    "#     cond_t  = f\"{cond}_twin\"            # 'condition_green_twin'\n",
    "#     col_h   = f\"{var}_human\"\n",
    "#     col_t   = f\"{var}_twin\"\n",
    "#     min_col = f\"{var}_min\"\n",
    "#     max_col = f\"{var}_max\"\n",
    "#     # always include the matching condition columns\n",
    "#     cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "#     pair = df[cols].dropna(subset=[col_h, col_t, cond_h, cond_t])\n",
    "# ####################################\n",
    "\n",
    "#     min_val = pair[min_col].iloc[0]\n",
    "#     max_val = pair[max_col].iloc[0]\n",
    "#     n    = len(pair)\n",
    "#     if n >= 4:\n",
    "#         r, _    = pearsonr(pair[col_h], pair[col_t])\n",
    "#         z_f     = np.arctanh(r)\n",
    "#         se      = 1 / np.sqrt(n - 3)\n",
    "#         z_crit  = norm.ppf(0.975)\n",
    "#         lo_z, hi_z = z_f - z_crit*se, z_f + z_crit*se\n",
    "#         lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "#         z_score    = z_f / se\n",
    "#         # Accuracy = mean absolute diff / range\n",
    "#         if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "#             accuracy = np.nan\n",
    "#         else:\n",
    "#             # compute mean absolute difference\n",
    "#             abs_diff      = np.abs(pair[col_h] - pair[col_t])\n",
    "#             mean_abs_diff = abs_diff.mean()\n",
    "#             accuracy      = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "#         mean_h = pair[col_h].mean()\n",
    "#         mean_t = pair[col_t].mean()\n",
    "\n",
    "#         # Paired t‐test\n",
    "#         t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "#         std_h = pair[col_h].std(ddof=1)\n",
    "#         std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "#          # F‐test for equal variances\n",
    "#         df1 = df2 = n - 1\n",
    "#         f_stat = (std_h**2 / std_t**2) if std_t>0 else np.nan\n",
    "\n",
    "#         # two‐tailed p‐value:\n",
    "#         if not np.isnan(f_stat):\n",
    "#             p_f = 2 * min(f.cdf(f_stat, df1, df2),\n",
    "#                           1 - f.cdf(f_stat, df1, df2))\n",
    "#         else:\n",
    "#             p_f = np.nan\n",
    "\n",
    "#         # Effect sizes (Cohen's d) across conditions\n",
    "#         #    For humans:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_h = pair[cond_h].unique()\n",
    "#             if len(levels_h) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_h]==levels_h[0], col_h]\n",
    "#                 g2 = pair.loc[pair[cond_h]==levels_h[1], col_h]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 # pooled sd\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_human = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_human = np.nan\n",
    "#         else:\n",
    "#             d_human = np.nan\n",
    "\n",
    "#         #    For twins:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_t = pair[cond_t].unique()\n",
    "#             if cond_exists and len(levels_t) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_t]==levels_t[0], col_t]\n",
    "#                 g2 = pair.loc[pair[cond_t]==levels_t[1], col_t]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_twin = np.nan\n",
    "#         else:\n",
    "#             d_twin = np.nan\n",
    "#     else:\n",
    "#         r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = f_stat = p_f = np.nan\n",
    "#         d_human = d_twin = np.nan\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         'study name': study_name,\n",
    "#         'variable name': var,\n",
    "#         'variable type (raw response/DV)':     'raw',\n",
    "#         'correlation between the responses from humans vs. their twins':        r,\n",
    "#         'CI_lower': lo_r,\n",
    "#         'CI_upper': hi_r,\n",
    "#         'z-score for correlation between humans vs. their twins':  z_score,\n",
    "#         'accuracy between humans vs. their twins': accuracy,\n",
    "#         'mean_human': mean_h,\n",
    "#         'mean_twin': mean_t,\n",
    "#         'paired t-test t-stat': t_stat,\n",
    "#         'paired t-test p-value': p_val,\n",
    "#         'std_human': std_h,\n",
    "#         'std_twin': std_t,\n",
    "#         'variance test F-stat': f_stat,\n",
    "#         'variance test p-value': p_f,\n",
    "#         'effect size based on human': d_human,\n",
    "#         'effect size based on twin': d_twin,\n",
    "#         'domain=social?':raw_vars_social_map.get(var, np.nan),\n",
    "#         'domain=cognitive?':raw_vars_cognitive_map.get(var, np.nan),\n",
    "#         'replicating know human bias?':raw_vars_known_map.get(var, np.nan),\n",
    "#         'preference measure?':raw_vars_pref_map.get(var, np.nan),\n",
    "#         'stimuli dependent?':raw_vars_stim_map.get(var, np.nan),\n",
    "#         'sample size':        n\n",
    "#     })\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "\n",
    "    dropped_ids = df.loc[df[col_h].isna() | df[col_t].isna(), \"TWIN_ID\"].unique()\n",
    "    if len(dropped_ids):\n",
    "        print(f\"{var}: dropping {len(dropped_ids)} pairs → TWIN_IDs: {dropped_ids.tolist()}\")\n",
    "    else:\n",
    "        print(f\"{var}: no observation dropped \")\n",
    "\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "#####participant-level data:\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "    # only keep non‑NaN values\n",
    "    long = long.dropna(subset=[\"value\"])\n",
    "\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "\n",
    "print(df_long.head())\n",
    "# save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffcbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
