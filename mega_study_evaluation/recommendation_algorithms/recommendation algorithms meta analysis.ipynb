{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b44df6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate found to rename.\n",
      "               study name persona specification                 variable name  \\\n",
      "0  recommendation systems       default persona                  usage_TikTok   \n",
      "1  recommendation systems       default persona                 usage_Netflix   \n",
      "2  recommendation systems       default persona         algo_knowledge_TikTok   \n",
      "3  recommendation systems       default persona        algo_knowledge_Netflix   \n",
      "4  recommendation systems       default persona   strategization_type1_TikTok   \n",
      "5  recommendation systems       default persona  strategization_type1_Netflix   \n",
      "6  recommendation systems       default persona   strategization_type2_TikTok   \n",
      "7  recommendation systems       default persona  strategization_type2_Netflix   \n",
      "8  recommendation systems       default persona           pref_control_TikTok   \n",
      "9  recommendation systems       default persona          pref_control_Netflix   \n",
      "\n",
      "   correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                           0.240736              0.116073   \n",
      "1                                           0.144550              0.046163   \n",
      "2                                           0.198426              0.072013   \n",
      "3                                           0.185941              0.088523   \n",
      "4                                           0.560498              0.465729   \n",
      "5                                           0.078117             -0.021094   \n",
      "6                                           0.214932              0.089143   \n",
      "7                                           0.171664              0.073872   \n",
      "8                                           0.116682             -0.011739   \n",
      "9                                           0.107437              0.008479   \n",
      "\n",
      "   CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0  0.357931                                           3.732113        \n",
      "1  0.240160                                           2.871090        \n",
      "2  0.318566                                           3.056362        \n",
      "3  0.279836                                           3.710502        \n",
      "4  0.642556                                           9.629263        \n",
      "5  0.175805                                           1.543855        \n",
      "6  0.333972                                           3.318424        \n",
      "7  0.266187                                           3.419606        \n",
      "8  0.241317                                           1.781533        \n",
      "9  0.204311                                           2.127200        \n",
      "\n",
      "   accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                 0.447650    2.089744   4.222222  ...   \n",
      "1                                 0.809949    2.081633   2.494898  ...   \n",
      "2                                 0.905983    0.901709   0.995726  ...   \n",
      "3                                 0.933673    0.931122   0.997449  ...   \n",
      "4                                 0.747863    0.529915   0.782051  ...   \n",
      "5                                 0.548469    0.543367   0.994898  ...   \n",
      "6                                 0.551282    0.508547   0.957265  ...   \n",
      "7                                 0.372449    0.311224   0.938776  ...   \n",
      "8                                 0.518519    1.991453   4.880342  ...   \n",
      "9                                 0.495748    1.862245   4.887755  ...   \n",
      "\n",
      "   effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                         NaN                        NaN               0   \n",
      "1                         NaN                        NaN               0   \n",
      "2                         NaN                        NaN               0   \n",
      "3                         NaN                        NaN               0   \n",
      "4                         NaN                        NaN               0   \n",
      "5                         NaN                        NaN               0   \n",
      "6                         NaN                        NaN               0   \n",
      "7                         NaN                        NaN               0   \n",
      "8                         NaN                        NaN               0   \n",
      "9                         NaN                        NaN               0   \n",
      "\n",
      "   domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                  0                             0                    0   \n",
      "1                  0                             0                    0   \n",
      "2                  0                             0                    0   \n",
      "3                  0                             0                    0   \n",
      "4                  0                             0                    0   \n",
      "5                  0                             0                    0   \n",
      "6                  0                             0                    0   \n",
      "7                  0                             0                    0   \n",
      "8                  0                             0                    1   \n",
      "9                  0                             0                    1   \n",
      "\n",
      "   stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                   0                    0                    0          234  \n",
      "1                   0                    0                    0          392  \n",
      "2                   0                    1                    0          234  \n",
      "3                   0                    1                    0          392  \n",
      "4                   0                    0                    0          234  \n",
      "5                   0                    0                    0          392  \n",
      "6                   0                    0                    0          234  \n",
      "7                   0                    0                    0          392  \n",
      "8                   0                    0                    0          234  \n",
      "9                   0                    0                    0          392  \n",
      "\n",
      "[10 rows x 26 columns]\n",
      "   TWIN_ID variable_name  value respondent_type              study_name  \\\n",
      "0        1  usage_TikTok    2.0           human  recommendation systems   \n",
      "1        3  usage_TikTok    1.0           human  recommendation systems   \n",
      "2       21  usage_TikTok    4.0           human  recommendation systems   \n",
      "3       22  usage_TikTok    1.0           human  recommendation systems   \n",
      "4       23  usage_TikTok    3.0           human  recommendation systems   \n",
      "\n",
      "  specification_name  \n",
      "0    default persona  \n",
      "1    default persona  \n",
      "2    default persona  \n",
      "3    default persona  \n",
      "4    default persona  \n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_35300\\633594206.py:83: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'failed attention check' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask_q13, 'exclusion'] = 'failed attention check'\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_35300\\633594206.py:83: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'failed attention check' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask_q13, 'exclusion'] = 'failed attention check'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "# Load data\n",
    "study_name = \"recommendation systems\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data values anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized.csv\"\n",
    "del df_human, df_twin\n",
    "df_human = pd.read_csv(human_file, header=0, skiprows=[1, 2])\n",
    "df_twin = pd.read_csv(twin_file, header=0, skiprows=[1, 2])\n",
    "\n",
    "# in df_human, rename the second column called 1_Q16_1 as 1_Q16_1.1 if needed\n",
    "# locate all positions of columns named '1_Q16_1'\n",
    "dup_idxs = [i for i, name in enumerate(df_human.columns) if name == \"1_Q16_1\"]\n",
    "# if there are at least two, rename the second one\n",
    "if len(dup_idxs) >= 2:\n",
    "    second_idx = dup_idxs[1]\n",
    "    df_human.columns.values[second_idx] = \"1_Q16_1.1\"\n",
    "    print(\"renamed duplicate columns in df_human\")\n",
    "else:\n",
    "    print(\"No duplicate found to rename.\")\n",
    "\n",
    "\n",
    "# add new columns with relevant variables coded\n",
    "usage_map = {\n",
    "    \"usage_TikTok\": \"1_Q15\",\n",
    "    \"usage_Netflix\": \"2_Q15\",\n",
    "    \"algo_knowledge_TikTok\": \"1_Q16_4\",\n",
    "    \"algo_knowledge_Netflix\": \"2_Q16_4\",\n",
    "    \"strategization_type1_TikTok\": \"1_Q16_1.1\",\n",
    "    \"strategization_type1_Netflix\": \"2_Q16_1.1\",\n",
    "}\n",
    "for df in (df_human, df_twin):\n",
    "    for new_col, src_col in usage_map.items():\n",
    "        df[new_col] = df[src_col].fillna(0)\n",
    "\n",
    "for df in (df_human, df_twin):\n",
    "    df[\"strategization_type2_TikTok\"] = ((df[\"1_Q18_5\"] == 1) | (df[\"1_Q18_7\"] == 1)).astype(int)\n",
    "for df in (df_human, df_twin):\n",
    "    df[\"strategization_type2_Netflix\"] = ((df[\"2_Q18_5\"] == 1) | (df[\"2_Q18_7\"] == 1)).astype(int)\n",
    "\n",
    "for df in (df_human, df_twin):\n",
    "    # TikTok: Q20_2.1 … Q20_6.1 plus Q20_7\n",
    "    tiktok_cols = [f\"1_Q20_{i}.1\" for i in range(2, 7)] + [\"1_Q20_7\"]\n",
    "    df[\"pref_control_TikTok\"] = df[tiktok_cols].fillna(0).sum(axis=1)\n",
    "    # Netflix: Q20_2.1 … Q20_6.1 plus Q20_7 (but with “2_” prefix)\n",
    "    netflix_cols = [f\"2_Q20_{i}.1\" for i in range(2, 7)] + [\"2_Q20_7\"]\n",
    "    df[\"pref_control_Netflix\"] = df[netflix_cols].fillna(0).sum(axis=1)\n",
    "\n",
    "\n",
    "# limit to pairs where both indicated usage of platform and where Q13=5 (attention check)\n",
    "# list all of the new columns you’ve created\n",
    "new_cols = [\n",
    "    \"usage_TikTok\",\n",
    "    \"usage_Netflix\",\n",
    "    \"algo_knowledge_TikTok\",\n",
    "    \"algo_knowledge_Netflix\",\n",
    "    \"strategization_type1_TikTok\",\n",
    "    \"strategization_type1_Netflix\",\n",
    "    \"strategization_type2_TikTok\",\n",
    "    \"strategization_type2_Netflix\",\n",
    "    \"pref_control_TikTok\",\n",
    "    \"pref_control_Netflix\",\n",
    "]\n",
    "\n",
    "# split out TikTok‑ vs. Netflix‑specific columns\n",
    "tik_tok_cols = [c for c in new_cols if \"TikTok\" in c]\n",
    "netflix_cols = [c for c in new_cols if \"Netflix\" in c]\n",
    "\n",
    "for df in (df_human, df_twin):\n",
    "    if \"exclusion\" not in df.columns:\n",
    "        df[\"exclusion\"] = np.nan\n",
    "\n",
    "# if Q13 ≠ 5 (failed attention check) in either human or twin, null out *all* new cols for that TWIN_ID\n",
    "mask_q13 = (df_human[\"Q13\"] != 5) | (df_twin[\"Q13\"] != 5)\n",
    "for df in (df_human, df_twin):\n",
    "    df.loc[mask_q13, new_cols] = np.nan\n",
    "    df.loc[mask_q13, \"exclusion\"] = \"failed attention check\"\n",
    "\n",
    "# TikTok missing → mask_use6\n",
    "mask_use6 = df_human[\"Platform Use_6\"].isna() | df_twin[\"Platform Use_6\"].isna()\n",
    "for df in (df_human, df_twin):\n",
    "    # null out the TikTok‐cols\n",
    "    df.loc[mask_use6, tik_tok_cols] = np.nan\n",
    "    # grab the existing text (or empty string)\n",
    "    existing = df[\"exclusion\"].fillna(\"\")\n",
    "    # build the new text: if there was already something, prepend \"; \"\n",
    "    suffix = np.where(existing != \"\", existing + \"; no TikTok usage\", \"no TikTok usage\")\n",
    "    # assign back, but only on the mask rows\n",
    "    df.loc[mask_use6, \"exclusion\"] = suffix[mask_use6]\n",
    "\n",
    "# Netflix missing → mask_use4\n",
    "mask_use4 = df_human[\"Platform Use_4\"].isna() | df_twin[\"Platform Use_4\"].isna()\n",
    "for df in (df_human, df_twin):\n",
    "    df.loc[mask_use4, netflix_cols] = np.nan\n",
    "    existing = df[\"exclusion\"].fillna(\"\")\n",
    "    suffix = np.where(existing != \"\", existing + \"; no Netflix usage\", \"no Netflix usage\")\n",
    "    df.loc[mask_use4, \"exclusion\"] = suffix[mask_use4]\n",
    "\n",
    "# # save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} human data pre-processed.csv\"\n",
    "df_human.to_csv(out_file, index=False)\n",
    "\n",
    "# # save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} twin data pre-processed.csv\"\n",
    "df_twin.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "\n",
    "# raw responses:\n",
    "raw_vars = []\n",
    "# raw_vars_min = []\n",
    "# raw_vars_max = []\n",
    "# #raw responses: domain=social?\n",
    "# raw_vars_social=[]\n",
    "# raw_vars_social_map = dict(zip(raw_vars, raw_vars_social))\n",
    "# #raw responses: domain=cognitive?\n",
    "# raw_vars_cognitive=[]\n",
    "# raw_vars_cognitive_map = dict(zip(raw_vars, raw_vars_cognitive))\n",
    "# #raw responses: replicating know human bias?\n",
    "# raw_vars_known=[]\n",
    "# raw_vars_known_map = dict(zip(raw_vars, raw_vars_known))\n",
    "# #raw responses: preference measure?\n",
    "# raw_vars_pref=[]\n",
    "# raw_vars_pref_map = dict(zip(raw_vars, raw_vars_pref))\n",
    "# #raw responses: stimuli dependent?\n",
    "# raw_vars_stim=[]\n",
    "# raw_vars_stim_map = dict(zip(raw_vars, raw_vars_stim))\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [\n",
    "    \"usage_TikTok\",\n",
    "    \"usage_Netflix\",\n",
    "    \"algo_knowledge_TikTok\",\n",
    "    \"algo_knowledge_Netflix\",\n",
    "    \"strategization_type1_TikTok\",\n",
    "    \"strategization_type1_Netflix\",\n",
    "    \"strategization_type2_TikTok\",\n",
    "    \"strategization_type2_Netflix\",\n",
    "    \"pref_control_TikTok\",\n",
    "    \"pref_control_Netflix\",\n",
    "]\n",
    "DV_vars_min = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "DV_vars_max = [5, 5, 1, 1, 1, 1, 1, 1, 6, 6]\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [0] * 10\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [0] * 10\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0] * 10\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [0] * 10\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0] * 10\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "# Fix dtypes\n",
    "for var in raw_vars + DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "# for var in raw_vars:\n",
    "#     ##############################\n",
    "#     #07/18/26: condition assignment different for each DV.\n",
    "# #     col_h = f\"{var}_human\"\n",
    "# #     col_t = f\"{var}_twin\"\n",
    "# #     min_col = f\"{var}_min\"\n",
    "# #     max_col = f\"{var}_max\"\n",
    "# #     if cond_exists:\n",
    "# #         cols = [col_h, col_t, cond_h, cond_t,min_col,max_col]\n",
    "# #     else:\n",
    "# #         cols = [col_h, col_t,min_col,max_col]\n",
    "# #     pair = (\n",
    "# #     df[cols]\n",
    "# #       .dropna(subset=[col_h, col_t])\n",
    "# #     )\n",
    "# # look up the right condition for this DV\n",
    "#     cond    = dv_to_cond[var]           # e.g. 'condition_green'\n",
    "#     cond_h  = f\"{cond}_human\"           # 'condition_green_human'\n",
    "#     cond_t  = f\"{cond}_twin\"            # 'condition_green_twin'\n",
    "#     col_h   = f\"{var}_human\"\n",
    "#     col_t   = f\"{var}_twin\"\n",
    "#     min_col = f\"{var}_min\"\n",
    "#     max_col = f\"{var}_max\"\n",
    "#     # always include the matching condition columns\n",
    "#     cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "#     pair = df[cols].dropna(subset=[col_h, col_t, cond_h, cond_t])\n",
    "# ####################################\n",
    "\n",
    "#     min_val = pair[min_col].iloc[0]\n",
    "#     max_val = pair[max_col].iloc[0]\n",
    "#     n    = len(pair)\n",
    "#     if n >= 4:\n",
    "#         r, _    = pearsonr(pair[col_h], pair[col_t])\n",
    "#         z_f     = np.arctanh(r)\n",
    "#         se      = 1 / np.sqrt(n - 3)\n",
    "#         z_crit  = norm.ppf(0.975)\n",
    "#         lo_z, hi_z = z_f - z_crit*se, z_f + z_crit*se\n",
    "#         lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "#         z_score    = z_f / se\n",
    "#         # Accuracy = mean absolute diff / range\n",
    "#         if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "#             accuracy = np.nan\n",
    "#         else:\n",
    "#             # compute mean absolute difference\n",
    "#             abs_diff      = np.abs(pair[col_h] - pair[col_t])\n",
    "#             mean_abs_diff = abs_diff.mean()\n",
    "#             accuracy      = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "#         mean_h = pair[col_h].mean()\n",
    "#         mean_t = pair[col_t].mean()\n",
    "\n",
    "#         # Paired t‐test\n",
    "#         t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "#         std_h = pair[col_h].std(ddof=1)\n",
    "#         std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "#          # F‐test for equal variances\n",
    "#         df1 = df2 = n - 1\n",
    "#         f_stat = (std_h**2 / std_t**2) if std_t>0 else np.nan\n",
    "\n",
    "#         # two‐tailed p‐value:\n",
    "#         if not np.isnan(f_stat):\n",
    "#             p_f = 2 * min(f.cdf(f_stat, df1, df2),\n",
    "#                           1 - f.cdf(f_stat, df1, df2))\n",
    "#         else:\n",
    "#             p_f = np.nan\n",
    "\n",
    "#         # Effect sizes (Cohen's d) across conditions\n",
    "#         #    For humans:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_h = pair[cond_h].unique()\n",
    "#             if len(levels_h) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_h]==levels_h[0], col_h]\n",
    "#                 g2 = pair.loc[pair[cond_h]==levels_h[1], col_h]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 # pooled sd\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_human = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_human = np.nan\n",
    "#         else:\n",
    "#             d_human = np.nan\n",
    "\n",
    "#         #    For twins:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_t = pair[cond_t].unique()\n",
    "#             if cond_exists and len(levels_t) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_t]==levels_t[0], col_t]\n",
    "#                 g2 = pair.loc[pair[cond_t]==levels_t[1], col_t]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_twin = np.nan\n",
    "#         else:\n",
    "#             d_twin = np.nan\n",
    "#     else:\n",
    "#         r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = f_stat = p_f = np.nan\n",
    "#         d_human = d_twin = np.nan\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         'study name': study_name,\n",
    "#         'variable name': var,\n",
    "#         'variable type (raw response/DV)':     'raw',\n",
    "#         'correlation between the responses from humans vs. their twins':        r,\n",
    "#         'CI_lower': lo_r,\n",
    "#         'CI_upper': hi_r,\n",
    "#         'z-score for correlation between humans vs. their twins':  z_score,\n",
    "#         'accuracy between humans vs. their twins': accuracy,\n",
    "#         'mean_human': mean_h,\n",
    "#         'mean_twin': mean_t,\n",
    "#         'paired t-test t-stat': t_stat,\n",
    "#         'paired t-test p-value': p_val,\n",
    "#         'std_human': std_h,\n",
    "#         'std_twin': std_t,\n",
    "#         'variance test F-stat': f_stat,\n",
    "#         'variance test p-value': p_f,\n",
    "#         'effect size based on human': d_human,\n",
    "#         'effect size based on twin': d_twin,\n",
    "#         'domain=social?':raw_vars_social_map.get(var, np.nan),\n",
    "#         'domain=cognitive?':raw_vars_cognitive_map.get(var, np.nan),\n",
    "#         'replicating know human bias?':raw_vars_known_map.get(var, np.nan),\n",
    "#         'preference measure?':raw_vars_pref_map.get(var, np.nan),\n",
    "#         'stimuli dependent?':raw_vars_stim_map.get(var, np.nan),\n",
    "#         'sample size':        n\n",
    "#     })\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "#####participant-level data:\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "    # only keep non‑NaN values\n",
    "    long = long.dropna(subset=[\"value\"])\n",
    "\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "\n",
    "print(df_long.head())\n",
    "# save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edade1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
