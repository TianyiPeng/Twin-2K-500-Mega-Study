{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cde7fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged df columns: ['StartDate_human', 'EndDate_human', 'Progress_human', 'Duration (in seconds)_human', 'Finished_human', 'RecordedDate_human', 'Browser_Browser_human', 'Browser_Version_human', 'Browser_Operating System_human', 'Browser_Resolution_human', 'Q275_First Click_human', 'Q275_Last Click_human', 'Q275_Page Submit_human', 'Q275_Click Count_human', 'Profile1_human', 'Profile2_human', 'Profile3_human', 'Profile4_human', 'Profile5_human', 'Profile6_human', 'Profile7_human', 'Profile8_human', 'Profile9_human', 'Profile10_human', 'Profile11_human', 'Profile12_human', 'Profile13_human', 'Profile14_human', 'Profile15_human', 'Profile16_human', 'Profile17_human', 'Profile18_human', 'Impt_ratings_1_human', 'Impt_ratings_2_human', 'Impt_ratings_3_human', 'Impt_ratings_4_human', 'Impt_ratings_5_human', 'Acc_ratings_1_human', 'Acc_ratings_2_human', 'Acc_ratings_3_human', 'Acc_ratings_4_human', 'Acc_ratings_5_human', 'MC_correct_entertain_1_human', 'TC_BBC_7_human', 'TC_PBS_7_human', 'TC_quora_7_human', 'TC_reddit_7_human', 'Q1572_First Click_human', 'Q1572_Last Click_human', 'Q1572_Page Submit_human', 'Q1572_Click Count_human', 'SocialMedia_Chk_human', 'Random_human', 'Google_human', 'Comments_human', 'TWIN_ID', 'StartDate_twin', 'EndDate_twin', 'Progress_twin', 'Duration (in seconds)_twin', 'Finished_twin', 'RecordedDate_twin', 'Browser_Browser_twin', 'Browser_Version_twin', 'Browser_Operating System_twin', 'Browser_Resolution_twin', 'Q275_First Click_twin', 'Q275_Last Click_twin', 'Q275_Page Submit_twin', 'Q275_Click Count_twin', 'Profile1_twin', 'Profile2_twin', 'Profile3_twin', 'Profile4_twin', 'Profile5_twin', 'Profile6_twin', 'Profile7_twin', 'Profile8_twin', 'Profile9_twin', 'Profile10_twin', 'Profile11_twin', 'Profile12_twin', 'Profile13_twin', 'Profile14_twin', 'Profile15_twin', 'Profile16_twin', 'Profile17_twin', 'Profile18_twin', 'Impt_ratings_1_twin', 'Impt_ratings_2_twin', 'Impt_ratings_3_twin', 'Impt_ratings_4_twin', 'Impt_ratings_5_twin', 'Acc_ratings_1_twin', 'Acc_ratings_2_twin', 'Acc_ratings_3_twin', 'Acc_ratings_4_twin', 'Acc_ratings_5_twin', 'MC_correct_entertain_1_twin', 'TC_BBC_7_twin', 'TC_PBS_7_twin', 'TC_quora_7_twin', 'TC_reddit_7_twin', 'Q1572_First Click_twin', 'Q1572_Last Click_twin', 'Q1572_Page Submit_twin', 'Q1572_Click Count_twin', 'SocialMedia_Chk_twin', 'Random_twin', 'Google_twin', 'Comments_twin']\n",
      "                   study name persona specification variable name  \\\n",
      "0   infotainment news sharing       default persona      Profile1   \n",
      "1   infotainment news sharing       default persona      Profile2   \n",
      "2   infotainment news sharing       default persona      Profile3   \n",
      "3   infotainment news sharing       default persona      Profile4   \n",
      "4   infotainment news sharing       default persona      Profile5   \n",
      "5   infotainment news sharing       default persona      Profile6   \n",
      "6   infotainment news sharing       default persona      Profile7   \n",
      "7   infotainment news sharing       default persona      Profile8   \n",
      "8   infotainment news sharing       default persona      Profile9   \n",
      "9   infotainment news sharing       default persona     Profile10   \n",
      "10  infotainment news sharing       default persona     Profile11   \n",
      "11  infotainment news sharing       default persona     Profile12   \n",
      "12  infotainment news sharing       default persona     Profile13   \n",
      "13  infotainment news sharing       default persona     Profile14   \n",
      "14  infotainment news sharing       default persona     Profile15   \n",
      "15  infotainment news sharing       default persona     Profile16   \n",
      "16  infotainment news sharing       default persona     Profile17   \n",
      "17  infotainment news sharing       default persona     Profile18   \n",
      "\n",
      "    correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                            0.328381              0.223450   \n",
      "1                                            0.191559              0.080055   \n",
      "2                                            0.261189              0.152457   \n",
      "3                                            0.297765              0.190963   \n",
      "4                                            0.219358              0.108820   \n",
      "5                                            0.223969              0.113610   \n",
      "6                                            0.183049              0.071286   \n",
      "7                                            0.206373              0.095361   \n",
      "8                                            0.300769              0.194140   \n",
      "9                                            0.292193              0.185076   \n",
      "10                                           0.176713              0.064769   \n",
      "11                                           0.098893             -0.014511   \n",
      "12                                           0.283596              0.176008   \n",
      "13                                           0.263013              0.154369   \n",
      "14                                           0.116645              0.003450   \n",
      "15                                           0.214331              0.103605   \n",
      "16                                           0.100864             -0.012520   \n",
      "17                                           0.110964             -0.002306   \n",
      "\n",
      "    CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0   0.425789                                           5.876908        \n",
      "1   0.298328                                           3.342560        \n",
      "2   0.363673                                           4.608005        \n",
      "3   0.397599                                           5.291862        \n",
      "4   0.324537                                           3.842788        \n",
      "5   0.328869                                           3.926374        \n",
      "6   0.290273                                           3.190574        \n",
      "7   0.312315                                           3.608391        \n",
      "8   0.400373                                           5.348727        \n",
      "9   0.392449                                           5.186698        \n",
      "10  0.284266                                           3.077731        \n",
      "11  0.209784                                           1.709875        \n",
      "12  0.384489                                           5.025144        \n",
      "13  0.365371                                           4.641758        \n",
      "14  0.226889                                           2.019415        \n",
      "15  0.319810                                           3.751886        \n",
      "16  0.211687                                           1.744190        \n",
      "17  0.221422                                           1.920220        \n",
      "\n",
      "    accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                  0.830476    2.243333   2.190000  ...   \n",
      "1                                  0.747143    3.616667   3.793333  ...   \n",
      "2                                  0.823333    2.370000   2.300000  ...   \n",
      "3                                  0.769524    2.320000   3.200000  ...   \n",
      "4                                  0.749048    3.523333   2.733333  ...   \n",
      "5                                  0.729524    3.366667   4.053333  ...   \n",
      "6                                  0.796667    2.543333   2.086667  ...   \n",
      "7                                  0.852857    2.066667   1.843333  ...   \n",
      "8                                  0.799524    2.286667   2.650000  ...   \n",
      "9                                  0.799048    2.383333   2.710000  ...   \n",
      "10                                 0.750000    3.673333   3.676667  ...   \n",
      "11                                 0.732381    2.730000   3.456667  ...   \n",
      "12                                 0.764762    2.560000   3.306667  ...   \n",
      "13                                 0.764286    3.223333   2.753333  ...   \n",
      "14                                 0.749048    2.610000   3.440000  ...   \n",
      "15                                 0.739524    2.626667   3.516667  ...   \n",
      "16                                 0.792381    2.163333   2.603333  ...   \n",
      "17                                 0.808571    2.243333   2.483333  ...   \n",
      "\n",
      "    effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                          NaN                        NaN               1   \n",
      "1                          NaN                        NaN               1   \n",
      "2                          NaN                        NaN               1   \n",
      "3                          NaN                        NaN               1   \n",
      "4                          NaN                        NaN               1   \n",
      "5                          NaN                        NaN               1   \n",
      "6                          NaN                        NaN               1   \n",
      "7                          NaN                        NaN               1   \n",
      "8                          NaN                        NaN               1   \n",
      "9                          NaN                        NaN               1   \n",
      "10                         NaN                        NaN               1   \n",
      "11                         NaN                        NaN               1   \n",
      "12                         NaN                        NaN               1   \n",
      "13                         NaN                        NaN               1   \n",
      "14                         NaN                        NaN               1   \n",
      "15                         NaN                        NaN               1   \n",
      "16                         NaN                        NaN               1   \n",
      "17                         NaN                        NaN               1   \n",
      "\n",
      "    domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                   0                             0                    1   \n",
      "1                   0                             0                    1   \n",
      "2                   0                             0                    1   \n",
      "3                   0                             0                    1   \n",
      "4                   0                             0                    1   \n",
      "5                   0                             0                    1   \n",
      "6                   0                             0                    1   \n",
      "7                   0                             0                    1   \n",
      "8                   0                             0                    1   \n",
      "9                   0                             0                    1   \n",
      "10                  0                             0                    1   \n",
      "11                  0                             0                    1   \n",
      "12                  0                             0                    1   \n",
      "13                  0                             0                    1   \n",
      "14                  0                             0                    1   \n",
      "15                  0                             0                    1   \n",
      "16                  0                             0                    1   \n",
      "17                  0                             0                    1   \n",
      "\n",
      "    stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                    0                    0                    0          300  \n",
      "1                    0                    0                    0          300  \n",
      "2                    0                    0                    0          300  \n",
      "3                    0                    0                    0          300  \n",
      "4                    0                    0                    0          300  \n",
      "5                    0                    0                    0          300  \n",
      "6                    0                    0                    0          300  \n",
      "7                    0                    0                    0          300  \n",
      "8                    0                    0                    0          300  \n",
      "9                    0                    0                    0          300  \n",
      "10                   0                    0                    0          300  \n",
      "11                   0                    0                    0          300  \n",
      "12                   0                    0                    0          300  \n",
      "13                   0                    0                    0          300  \n",
      "14                   0                    0                    0          300  \n",
      "15                   0                    0                    0          300  \n",
      "16                   0                    0                    0          300  \n",
      "17                   0                    0                    0          300  \n",
      "\n",
      "[18 rows x 26 columns]\n",
      "   TWIN_ID variable_name  value respondent_type                 study_name  \\\n",
      "0        2      Profile1      1           human  infotainment news sharing   \n",
      "1        3      Profile1      1           human  infotainment news sharing   \n",
      "2        4      Profile1      1           human  infotainment news sharing   \n",
      "3       11      Profile1      1           human  infotainment news sharing   \n",
      "4       13      Profile1      1           human  infotainment news sharing   \n",
      "\n",
      "  specification_name  \n",
      "0    default persona  \n",
      "1    default persona  \n",
      "2    default persona  \n",
      "3    default persona  \n",
      "4    default persona  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "# Load data\n",
    "study_name = \"infotainment news sharing\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data values anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized.csv\"\n",
    "df_human = pd.read_csv(human_file, header=0, skiprows=[1, 2])\n",
    "df_twin = pd.read_csv(twin_file, header=0, skiprows=[1, 2])\n",
    "\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "\n",
    "# raw responses:\n",
    "raw_vars = []\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [f\"Profile{i}\" for i in range(1, 19)]\n",
    "DV_vars_min = [1] * 18\n",
    "DV_vars_max = [8] * 18\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [1] * 18\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [0] * 18\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0] * 18\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [1] * 18\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [0] * 18\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [0] * 18\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0] * 18\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "print(\"Merged df columns:\", df.columns.tolist())\n",
    "\n",
    "# Fix dtypes\n",
    "for var in raw_vars + DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "################################################\n",
    "\n",
    "\n",
    "# print results:\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "##### participant‐level data\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them and keep only non-NaN values\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "df_long = df_long[df_long[\"value\"].notna()]\n",
    "\n",
    "print(df_long.head())\n",
    "\n",
    "# save output as csv - unit of observation is TWIN_ID (only non-NaNs)\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0125e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
