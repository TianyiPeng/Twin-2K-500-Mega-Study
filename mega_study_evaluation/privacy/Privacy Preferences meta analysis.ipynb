{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3743548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ot2107\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate 'Q2_1' found to rename.\n",
      "  study name persona specification variable name  \\\n",
      "0    Privacy       default persona           PPV   \n",
      "\n",
      "   correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                           0.584777              0.546265   \n",
      "\n",
      "   CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0  0.620823                                          23.169808        \n",
      "\n",
      "   accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                 0.740694      3.5075   2.171667  ...   \n",
      "\n",
      "   effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                         NaN                        NaN               1   \n",
      "\n",
      "   domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                  0                             0                    1   \n",
      "\n",
      "   stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                   1                    0                    0         1200  \n",
      "\n",
      "[1 rows x 26 columns]\n",
      "   TWIN_ID variable_name  value respondent_type study_name specification_name\n",
      "0        1           PPV    1.0           human    Privacy    default persona\n",
      "1        2           PPV    3.0           human    Privacy    default persona\n",
      "2        3           PPV    3.0           human    Privacy    default persona\n",
      "3        4           PPV    1.0           human    Privacy    default persona\n",
      "4        5           PPV    7.0           human    Privacy    default persona\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "# Load data\n",
    "study_name = \"Privacy\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data values anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized.csv\"\n",
    "df_human = pd.read_csv(human_file, header=0, skiprows=[1, 2])\n",
    "df_twin = pd.read_csv(twin_file, header=0, skiprows=[1, 2])\n",
    "\n",
    "# in df_human, rename the second column called Q2_1 as Q2_1.1 if needed\n",
    "# locate all positions of columns named 'Q2_1'\n",
    "dup_idxs = [i for i, name in enumerate(df_human.columns) if name == \"Q2_1\"]\n",
    "# if there are at least two, rename the second one\n",
    "if len(dup_idxs) >= 2:\n",
    "    second_idx = dup_idxs[1]\n",
    "    df_human.columns.values[second_idx] = \"Q2_1.1\"\n",
    "    print(\"renamed duplicate 'Q2_1'\")\n",
    "else:\n",
    "    print(\"No duplicate 'Q2_1' found to rename.\")\n",
    "\n",
    "\n",
    "# create new variable PPV\n",
    "# Define a regex to pick up exactly Q11_1, Q138_1, Q140_1, Q142_1, Q2_1 and Q2_1.1 columns\n",
    "ppv_pattern = r\"^(?:Q11_1|Q138_1|Q140_1|Q142_1|Q2_1|Q2_1\\.1)$\"\n",
    "\n",
    "for df in (df_human, df_twin):\n",
    "    # 1) identify the six PPV columns (this will pick up both copies of Q2_1 if they exist)\n",
    "    ppv_cols = df.filter(regex=ppv_pattern).columns.tolist()\n",
    "    if len(ppv_cols) != 6:\n",
    "        raise ValueError(f\"Expected 6 PPV columns, found {len(ppv_cols)}: {ppv_cols}\")\n",
    "\n",
    "    # 2) check that exactly one is non-NaN per row\n",
    "    non_na_counts = df[ppv_cols].notna().sum(axis=1)\n",
    "    bad = non_na_counts != 1\n",
    "    if bad.any():\n",
    "        # report which rows violate the rule\n",
    "        idx = df.index[bad].tolist()\n",
    "        counts = non_na_counts[bad].tolist()\n",
    "        raise ValueError(\n",
    "            f\"Rows {idx} have non-NaN counts {counts} (must be exactly 1) among {ppv_cols}\"\n",
    "        )\n",
    "\n",
    "    # 3) create PPV as the sum (only the single non-NaN will contribute)\n",
    "    df[\"PPV\"] = df[ppv_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"Group\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "# #raw responses:\n",
    "# raw_vars = ['PPV']\n",
    "# raw_vars_min = [1]\n",
    "# raw_vars_max = [7]\n",
    "# #raw responses: domain=social?\n",
    "# raw_vars_social=[1]\n",
    "# raw_vars_social_map = dict(zip(raw_vars, raw_vars_social))\n",
    "# #raw responses: domain=cognitive?\n",
    "# raw_vars_cognitive=[0]\n",
    "# raw_vars_cognitive_map = dict(zip(raw_vars, raw_vars_cognitive))\n",
    "# #raw responses: replicating know human bias?\n",
    "# raw_vars_known=[0]\n",
    "# raw_vars_known_map = dict(zip(raw_vars, raw_vars_known))\n",
    "# #raw responses: preference measure?\n",
    "# raw_vars_pref=[1]\n",
    "# raw_vars_pref_map = dict(zip(raw_vars, raw_vars_pref))\n",
    "# #raw responses: stimuli dependent?\n",
    "# raw_vars_stim=[1]\n",
    "# raw_vars_stim_map = dict(zip(raw_vars, raw_vars_stim))\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [\"PPV\"]\n",
    "DV_vars_min = [1]\n",
    "DV_vars_max = [7]\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [1]\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [0]\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0]\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [1]\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [1]\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [0]\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0]\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "# Fix dtypes\n",
    "# for var in raw_vars + DV_vars:\n",
    "for var in DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "# for var in raw_vars:\n",
    "#     col_h = f\"{var}_human\"\n",
    "#     col_t = f\"{var}_twin\"\n",
    "#     min_col = f\"{var}_min\"\n",
    "#     max_col = f\"{var}_max\"\n",
    "#     if cond_exists:\n",
    "#         cols = [col_h, col_t, cond_h, cond_t,min_col,max_col]\n",
    "#     else:\n",
    "#         cols = [col_h, col_t,min_col,max_col]\n",
    "#     pair = (\n",
    "#     df[cols]\n",
    "#       .dropna(subset=[col_h, col_t])\n",
    "#     )\n",
    "#     min_val = pair[min_col].iloc[0]\n",
    "#     max_val = pair[max_col].iloc[0]\n",
    "#     n    = len(pair)\n",
    "#     if n >= 4:\n",
    "#         r, _    = pearsonr(pair[col_h], pair[col_t])\n",
    "#         z_f     = np.arctanh(r)\n",
    "#         se      = 1 / np.sqrt(n - 3)\n",
    "#         z_crit  = norm.ppf(0.975)\n",
    "#         lo_z, hi_z = z_f - z_crit*se, z_f + z_crit*se\n",
    "#         lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "#         z_score    = z_f / se\n",
    "#         # Accuracy = mean absolute diff / range\n",
    "#         if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "#             accuracy = np.nan\n",
    "#         else:\n",
    "#             # compute mean absolute difference\n",
    "#             abs_diff      = np.abs(pair[col_h] - pair[col_t])\n",
    "#             mean_abs_diff = abs_diff.mean()\n",
    "#             accuracy      = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "#         mean_h = pair[col_h].mean()\n",
    "#         mean_t = pair[col_t].mean()\n",
    "\n",
    "#         # Paired t‐test\n",
    "#         t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "#         std_h = pair[col_h].std(ddof=1)\n",
    "#         std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "#          # F‐test for equal variances\n",
    "#         df1 = df2 = n - 1\n",
    "#         f_stat = (std_h**2 / std_t**2) if std_t>0 else np.nan\n",
    "\n",
    "#         # two‐tailed p‐value:\n",
    "#         if not np.isnan(f_stat):\n",
    "#             p_f = 2 * min(f.cdf(f_stat, df1, df2),\n",
    "#                           1 - f.cdf(f_stat, df1, df2))\n",
    "#         else:\n",
    "#             p_f = np.nan\n",
    "\n",
    "#         # Effect sizes (Cohen's d) across conditions\n",
    "#         #    For humans:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_h = pair[cond_h].unique()\n",
    "#             if len(levels_h) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_h]==levels_h[0], col_h]\n",
    "#                 g2 = pair.loc[pair[cond_h]==levels_h[1], col_h]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 # pooled sd\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_human = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_human = np.nan\n",
    "#         else:\n",
    "#             d_human = np.nan\n",
    "\n",
    "#         #    For twins:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_t = pair[cond_t].unique()\n",
    "#             if cond_exists and len(levels_t) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_t]==levels_t[0], col_t]\n",
    "#                 g2 = pair.loc[pair[cond_t]==levels_t[1], col_t]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_twin = np.nan\n",
    "#         else:\n",
    "#             d_twin = np.nan\n",
    "#     else:\n",
    "#         r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = f_stat = p_f = np.nan\n",
    "#         d_human = d_twin = np.nan\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         'study name': study_name,\n",
    "#         'variable name': var,\n",
    "#         'variable type (raw response/DV)':     'raw',\n",
    "#         'correlation between the responses from humans vs. their twins':        r,\n",
    "#         'CI_lower': lo_r,\n",
    "#         'CI_upper': hi_r,\n",
    "#         'z-score for correlation between humans vs. their twins':  z_score,\n",
    "#         'accuracy between humans vs. their twins': accuracy,\n",
    "#         'mean_human': mean_h,\n",
    "#         'mean_twin': mean_t,\n",
    "#         'paired t-test t-stat': t_stat,\n",
    "#         'paired t-test p-value': p_val,\n",
    "#         'std_human': std_h,\n",
    "#         'std_twin': std_t,\n",
    "#         'variance test F-stat': f_stat,\n",
    "#         'variance test p-value': p_f,\n",
    "#         'effect size based on human': d_human,\n",
    "#         'effect size based on twin': d_twin,\n",
    "#         'domain=social?':raw_vars_social_map.get(var, np.nan),\n",
    "#         'domain=cognitive?':raw_vars_cognitive_map.get(var, np.nan),\n",
    "#         'replicating know human bias?':raw_vars_known_map.get(var, np.nan),\n",
    "#         'preference measure?':raw_vars_pref_map.get(var, np.nan),\n",
    "#         'stimuli dependent?':raw_vars_stim_map.get(var, np.nan),\n",
    "#         'sample size':        n\n",
    "#     })\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "#####participant-level data:\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "\n",
    "print(df_long.head())\n",
    "# save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03463dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variable  N_pairs  Mean_human  Mean_twin  CI95_hu_low  CI95_hu_high  \\\n",
      "0   Q2_1.1      199    1.743719   1.000000     1.552640      1.934797   \n",
      "1     Q2_1      218    2.133028   1.000000     1.935499      2.330556   \n",
      "2    Q11_1      187    2.657754   1.000000     2.396047      2.919461   \n",
      "3   Q140_1      222    4.666667   2.711712     4.443463      4.889870   \n",
      "4   Q142_1      180    4.461111   2.816667     4.213643      4.708579   \n",
      "5   Q138_1      194    5.469072   4.603093     5.245682      5.692462   \n",
      "\n",
      "   CI95_tw_low  CI95_tw_high  Pearson_r   p-val_r     t_stat       p-val_t  \\\n",
      "0     1.000000      1.000000        NaN       NaN   7.675527  7.357661e-13   \n",
      "1     1.000000      1.000000        NaN       NaN  11.305422  1.329006e-23   \n",
      "2     1.000000      1.000000        NaN       NaN  12.496474  2.067200e-26   \n",
      "3     2.616071      2.807353   0.013597  0.840340  15.944605  1.320809e-38   \n",
      "4     2.726715      2.906618  -0.069404  0.354563  12.058130  6.839914e-25   \n",
      "5     4.475604      4.730582   0.087987  0.222480   6.907297  6.972936e-11   \n",
      "\n",
      "      F_var     p-value_F  \n",
      "0       NaN           NaN  \n",
      "1       NaN           NaN  \n",
      "2       NaN           NaN  \n",
      "3  5.446437  7.885618e-33  \n",
      "4  7.568749  2.841058e-36  \n",
      "5  3.070315  3.160514e-14  \n",
      "Spearman rank correlation between mean_human and mean_twin: rho = 0.880, p-value = 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ot2107\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "C:\\Users\\ot2107\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "C:\\Users\\ot2107\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# replicate pre-registered analysis:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# (1) List of variables to summarize\n",
    "vars_to_summarize = [\"Q2_1.1\", \"Q2_1\", \"Q11_1\", \"Q140_1\", \"Q142_1\", \"Q138_1\"]\n",
    "\n",
    "# (2) Prepare a place to collect results\n",
    "rows = []\n",
    "\n",
    "for var in vars_to_summarize:\n",
    "    # merge on TWIN_ID and keep only pairs where both have a value\n",
    "    merged = (\n",
    "        df_human[[\"TWIN_ID\", var]]\n",
    "        .merge(df_twin[[\"TWIN_ID\", var]], on=\"TWIN_ID\", suffixes=(\"_hu\", \"_tw\"))\n",
    "        .dropna(subset=[f\"{var}_hu\", f\"{var}_tw\"])\n",
    "    )\n",
    "    n = len(merged)\n",
    "\n",
    "    # means\n",
    "    mean_h = merged[f\"{var}_hu\"].mean()\n",
    "    mean_t = merged[f\"{var}_tw\"].mean()\n",
    "\n",
    "    # 95% CI around each mean\n",
    "    if n > 1:\n",
    "        se_h = merged[f\"{var}_hu\"].std(ddof=1) / np.sqrt(n)\n",
    "        se_t = merged[f\"{var}_tw\"].std(ddof=1) / np.sqrt(n)\n",
    "        tcrit = stats.t.ppf(0.975, df=n - 1)\n",
    "        ci_h_l = mean_h - tcrit * se_h\n",
    "        ci_h_u = mean_h + tcrit * se_h\n",
    "        ci_t_l = mean_t - tcrit * se_t\n",
    "        ci_t_u = mean_t + tcrit * se_t\n",
    "    else:\n",
    "        ci_h_l = ci_h_u = ci_t_l = ci_t_u = np.nan\n",
    "\n",
    "    # Pearson r\n",
    "    if n > 1:\n",
    "        r, p_r = stats.pearsonr(merged[f\"{var}_hu\"], merged[f\"{var}_tw\"])\n",
    "        t_stat, p_t = stats.ttest_rel(merged[f\"{var}_hu\"], merged[f\"{var}_tw\"])\n",
    "    else:\n",
    "        r = p_r = t_stat = p_t = np.nan\n",
    "\n",
    "    # F‐test for equal variances\n",
    "    var_h = merged[f\"{var}_hu\"].var(ddof=1)\n",
    "    var_t = merged[f\"{var}_tw\"].var(ddof=1)\n",
    "    if n > 1 and var_h > 0 and var_t > 0:\n",
    "        # always put the larger variance in the numerator\n",
    "        if var_h >= var_t:\n",
    "            F = var_h / var_t\n",
    "        else:\n",
    "            F = var_t / var_h\n",
    "        df1 = df2 = n - 1\n",
    "        p_F = 2 * min(stats.f.cdf(F, df1, df2), stats.f.sf(F, df1, df2))\n",
    "    else:\n",
    "        F = p_F = np.nan\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Variable\": var,\n",
    "            \"N_pairs\": n,\n",
    "            \"Mean_human\": mean_h,\n",
    "            \"Mean_twin\": mean_t,\n",
    "            \"CI95_hu_low\": ci_h_l,\n",
    "            \"CI95_hu_high\": ci_h_u,\n",
    "            \"CI95_tw_low\": ci_t_l,\n",
    "            \"CI95_tw_high\": ci_t_u,\n",
    "            \"Pearson_r\": r,\n",
    "            \"p-val_r\": p_r,\n",
    "            \"t_stat\": t_stat,\n",
    "            \"p-val_t\": p_t,\n",
    "            \"F_var\": F,\n",
    "            \"p-value_F\": p_F,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# (3) Build and show the table\n",
    "summary_df = pd.DataFrame(rows)\n",
    "print(summary_df)\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute Spearman rank correlation between mean_human and mean_twin\n",
    "rho, p_value = spearmanr(summary_df[\"Mean_human\"], summary_df[\"Mean_twin\"], nan_policy=\"omit\")\n",
    "print(\n",
    "    f\"Spearman rank correlation between mean_human and mean_twin: \"\n",
    "    f\"rho = {rho:.3f}, p-value = {p_value:.3g}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e1b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
