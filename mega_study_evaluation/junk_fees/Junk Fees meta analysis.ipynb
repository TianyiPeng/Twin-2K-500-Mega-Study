{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53182399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ot2107\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "  study name persona specification     variable name  \\\n",
      "0  Junk Fees       default persona   percent_correct   \n",
      "1  Junk Fees       default persona  fairness_average   \n",
      "2  Junk Fees       default persona       reg_support   \n",
      "\n",
      "   correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                          -0.012276             -0.110195   \n",
      "1                                           0.286766              0.194174   \n",
      "2                                           0.478548              0.399229   \n",
      "\n",
      "   CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0  0.085880                                          -0.244600        \n",
      "1  0.374293                                           5.878610        \n",
      "2  0.550757                                          10.382837        \n",
      "\n",
      "   accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                 0.518750   51.750000  99.875000  ...   \n",
      "1                                 0.835764    3.280417   3.271667  ...   \n",
      "2                                 0.795417    5.027500   5.462500  ...   \n",
      "\n",
      "   effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                         NaN                        NaN               0   \n",
      "1                         NaN                        NaN               1   \n",
      "2                         NaN                        NaN               1   \n",
      "\n",
      "   domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                  1                             0                    0   \n",
      "1                  0                             0                    1   \n",
      "2                  0                             0                    1   \n",
      "\n",
      "   stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                   1                    1                    0          400  \n",
      "1                   1                    0                    0          400  \n",
      "2                   1                    0                    1          400  \n",
      "\n",
      "[3 rows x 26 columns]\n",
      "   TWIN_ID    variable_name      value respondent_type study_name  \\\n",
      "0        1  percent_correct  66.666667           human  Junk Fees   \n",
      "1        2  percent_correct  66.666667           human  Junk Fees   \n",
      "2        3  percent_correct  50.000000           human  Junk Fees   \n",
      "3        4  percent_correct  50.000000           human  Junk Fees   \n",
      "4        5  percent_correct  83.333333           human  Junk Fees   \n",
      "\n",
      "  specification_name  \n",
      "0    default persona  \n",
      "1    default persona  \n",
      "2    default persona  \n",
      "3    default persona  \n",
      "4    default persona  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "!pip install --upgrade gensim\n",
    "import re\n",
    "from itertools import chain, permutations\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine, euclidean, pdist\n",
    "\n",
    "# Load data\n",
    "study_name = \"Junk Fees\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data values anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized.csv\"\n",
    "df_human = pd.read_csv(human_file, header=0, skiprows=[1, 2])\n",
    "df_twin = pd.read_csv(twin_file, header=0, skiprows=[1, 2])\n",
    "\n",
    "\n",
    "#########################\n",
    "# create new relevant columns:\n",
    "\n",
    "# human:\n",
    "categories = [\"Hotel\", \"Car\", \"Ticket\", \"Food\", \"Apart\", \"Health\", \"CC\"]\n",
    "\n",
    "for cat in categories:\n",
    "    # find all MCQ columns for this category\n",
    "    mcq_cols = [col for col in df_human.columns if col.startswith(f\"{cat}.MCQ\")]\n",
    "    if not mcq_cols:\n",
    "        raise ValueError(f\"No {cat}.MCQ column found\")\n",
    "    mcq_col = mcq_cols[0]\n",
    "    correct_col = f\"{cat}.correct\"\n",
    "    df_human[correct_col] = np.where(\n",
    "        df_human[mcq_col].isna(), np.nan, (df_human[mcq_col] == 1).astype(int)\n",
    "    )\n",
    "    df_twin[correct_col] = np.where(\n",
    "        df_twin[mcq_col].isna(), np.nan, (df_twin[mcq_col] == 1).astype(int)\n",
    "    )\n",
    "\n",
    "    fair_cols = [col for col in df_human.columns if col.startswith(f\"{cat}.Fair\")]\n",
    "    if not fair_cols:\n",
    "        raise ValueError(f\"No {cat}.Fair column found\")\n",
    "    fair_col = fair_cols[0]\n",
    "    rating_col = f\"{cat}.fairness\"\n",
    "\n",
    "    df_human[rating_col] = df_human[fair_col]\n",
    "    df_twin[rating_col] = df_twin[fair_col]\n",
    "\n",
    "correct_cols = [f\"{cat}.correct\" for cat in categories]\n",
    "non_nan_counts = df_human[correct_cols].notna().sum(axis=1)\n",
    "if not (non_nan_counts == 6).all():\n",
    "    # report rows that don’t meet the expectation\n",
    "    bad_rows = non_nan_counts[non_nan_counts != 6]\n",
    "    raise ValueError(\n",
    "        f\"Expected exactly 6 non-NaN .correct columns per row, but found counts:\\n{bad_rows}\"\n",
    "    )\n",
    "non_nan_counts = df_twin[correct_cols].notna().sum(axis=1)\n",
    "if not (non_nan_counts == 6).all():\n",
    "    # report rows that don’t meet the expectation\n",
    "    bad_rows = non_nan_counts[non_nan_counts != 6]\n",
    "    raise ValueError(\n",
    "        f\"Expected exactly 6 non-NaN .correct columns per row, but found counts:\\n{bad_rows}\"\n",
    "    )\n",
    "fairness_cols = [f\"{cat}.fairness\" for cat in categories]\n",
    "non_nan_counts = df_human[fairness_cols].notna().sum(axis=1)\n",
    "if not (non_nan_counts == 6).all():\n",
    "    bad = non_nan_counts[non_nan_counts != 6]\n",
    "    raise ValueError(f\"Expected exactly 6 non-NaN fairness ratings per row, but found:\\n{bad}\")\n",
    "non_nan_counts = df_twin[fairness_cols].notna().sum(axis=1)\n",
    "if not (non_nan_counts == 6).all():\n",
    "    bad = non_nan_counts[non_nan_counts != 6]\n",
    "    raise ValueError(f\"Expected exactly 6 non-NaN fairness ratings per row, but found:\\n{bad}\")\n",
    "df_human[\"percent_correct\"] = df_human[correct_cols].sum(axis=1) / 6 * 100\n",
    "df_human[\"fairness_average\"] = df_human[fairness_cols].mean(axis=1)\n",
    "df_twin[\"percent_correct\"] = df_twin[correct_cols].sum(axis=1) / 6 * 100\n",
    "df_twin[\"fairness_average\"] = df_twin[fairness_cols].mean(axis=1)\n",
    "\n",
    "reg_cols = [\"Regulation\", \"Support.Reg\"]\n",
    "df_human[\"reg_support\"] = df_human[reg_cols].mean(axis=1)\n",
    "df_twin[\"reg_support\"] = df_twin[reg_cols].mean(axis=1)\n",
    "\n",
    "###############################\n",
    "\n",
    "\n",
    "human_file = f\"{study_name} human data values anonymized processed.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized processed.csv\"\n",
    "df_twin.to_csv(twin_file, index=False)\n",
    "df_human.to_csv(human_file, index=False)\n",
    "\n",
    "df_human = df_human.set_index(\"TWIN_ID\")\n",
    "df_twin = df_twin.set_index(\"TWIN_ID\")\n",
    "\n",
    "\n",
    "##################now proceed with standard meta-analysis steps\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "# raw responses:\n",
    "# # all the “raw” columns\n",
    "# raw_vars = list(chain(correct_cols, fairness_cols, reg_cols))\n",
    "# # check\n",
    "# print(len(raw_vars), \"columns:\", raw_vars)\n",
    "# raw_vars_min=list(chain([0]*7, [1]*7, [1]*2))\n",
    "# raw_vars_max = list(chain([1]*7, [7]*7, [7]*2))\n",
    "# #raw responses: domain=social?\n",
    "# raw_vars_social=list(chain([0]*7, [1]*7, [1]*2))\n",
    "# raw_vars_social_map = dict(zip(raw_vars, raw_vars_social))\n",
    "# #raw responses: domain=cognitive?\n",
    "# raw_vars_cognitive=list(chain([1]*7, [0]*7, [0]*2))\n",
    "# raw_vars_cognitive_map = dict(zip(raw_vars, raw_vars_cognitive))\n",
    "# #raw responses: replicating know human bias?\n",
    "# raw_vars_known=[0]*16\n",
    "# raw_vars_known_map = dict(zip(raw_vars, raw_vars_known))\n",
    "# #raw responses: preference measure?\n",
    "# raw_vars_pref=list(chain([0]*7, [1]*7, [1]*2))\n",
    "# raw_vars_pref_map = dict(zip(raw_vars, raw_vars_pref))\n",
    "# #raw responses: stimuli dependent?\n",
    "# raw_vars_stim=[1]*16\n",
    "# raw_vars_stim_map = dict(zip(raw_vars, raw_vars_stim))\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [\"percent_correct\", \"fairness_average\", \"reg_support\"]\n",
    "DV_vars_min = [0, 1, 1]\n",
    "# fixed range on 07/25/25:\n",
    "# DV_vars_max=[6,7,7]\n",
    "DV_vars_max = [100, 7, 7]\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [0, 1, 1]\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [1, 0, 0]\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0, 0, 0]\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [0, 1, 1]\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [1, 1, 1]\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [1, 0, 0]\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0, 0, 1]\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "# Fix dtypes\n",
    "# for var in raw_vars + DV_vars:\n",
    "for var in DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "# for var in raw_vars:\n",
    "#     col_h = f\"{var}_human\"\n",
    "#     col_t = f\"{var}_twin\"\n",
    "#     min_col = f\"{var}_min\"\n",
    "#     max_col = f\"{var}_max\"\n",
    "#     if cond_exists:\n",
    "#         cols = [col_h, col_t, cond_h, cond_t,min_col,max_col]\n",
    "#     else:\n",
    "#         cols = [col_h, col_t,min_col,max_col]\n",
    "#     pair = (\n",
    "#     df[cols]\n",
    "#       .dropna(subset=[col_h, col_t])\n",
    "#     )\n",
    "#     min_val = pair[min_col].iloc[0]\n",
    "#     max_val = pair[max_col].iloc[0]\n",
    "#     n    = len(pair)\n",
    "#     if n >= 4:\n",
    "#         r, _    = pearsonr(pair[col_h], pair[col_t])\n",
    "#         z_f     = np.arctanh(r)\n",
    "#         se      = 1 / np.sqrt(n - 3)\n",
    "#         z_crit  = norm.ppf(0.975)\n",
    "#         lo_z, hi_z = z_f - z_crit*se, z_f + z_crit*se\n",
    "#         lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "#         z_score    = z_f / se\n",
    "#         # Accuracy = mean absolute diff / range\n",
    "#         if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "#             accuracy = np.nan\n",
    "#         else:\n",
    "#             # compute mean absolute difference\n",
    "#             abs_diff      = np.abs(pair[col_h] - pair[col_t])\n",
    "#             mean_abs_diff = abs_diff.mean()\n",
    "#             accuracy      = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "#         mean_h = pair[col_h].mean()\n",
    "#         mean_t = pair[col_t].mean()\n",
    "\n",
    "#         # Paired t‐test\n",
    "#         t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "#         std_h = pair[col_h].std(ddof=1)\n",
    "#         std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "#          # F‐test for equal variances\n",
    "#         df1 = df2 = n - 1\n",
    "#         f_stat = (std_h**2 / std_t**2) if std_t>0 else np.nan\n",
    "\n",
    "#         # two‐tailed p‐value:\n",
    "#         if not np.isnan(f_stat):\n",
    "#             p_f = 2 * min(f.cdf(f_stat, df1, df2),\n",
    "#                           1 - f.cdf(f_stat, df1, df2))\n",
    "#         else:\n",
    "#             p_f = np.nan\n",
    "\n",
    "#         # Effect sizes (Cohen's d) across conditions\n",
    "#         #    For humans:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_h = pair[cond_h].unique()\n",
    "#             if len(levels_h) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_h]==levels_h[0], col_h]\n",
    "#                 g2 = pair.loc[pair[cond_h]==levels_h[1], col_h]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 # pooled sd\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_human = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_human = np.nan\n",
    "#         else:\n",
    "#             d_human = np.nan\n",
    "\n",
    "#         #    For twins:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_t = pair[cond_t].unique()\n",
    "#             if cond_exists and len(levels_t) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_t]==levels_t[0], col_t]\n",
    "#                 g2 = pair.loc[pair[cond_t]==levels_t[1], col_t]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_twin = np.nan\n",
    "#         else:\n",
    "#             d_twin = np.nan\n",
    "#     else:\n",
    "#         r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = f_stat = p_f = np.nan\n",
    "#         d_human = d_twin = np.nan\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         'study name': study_name,\n",
    "#         'variable name': var,\n",
    "#         'variable type (raw response/DV)':     'raw',\n",
    "#         'correlation between the responses from humans vs. their twins':        r,\n",
    "#         'CI_lower': lo_r,\n",
    "#         'CI_upper': hi_r,\n",
    "#         'z-score for correlation between humans vs. their twins':  z_score,\n",
    "#         'accuracy between humans vs. their twins': accuracy,\n",
    "#         'mean_human': mean_h,\n",
    "#         'mean_twin': mean_t,\n",
    "#         'paired t-test t-stat': t_stat,\n",
    "#         'paired t-test p-value': p_val,\n",
    "#         'std_human': std_h,\n",
    "#         'std_twin': std_t,\n",
    "#         'variance test F-stat': f_stat,\n",
    "#         'variance test p-value': p_f,\n",
    "#         'effect size based on human': d_human,\n",
    "#         'effect size based on twin': d_twin,\n",
    "#         'domain=social?':raw_vars_social_map.get(var, np.nan),\n",
    "#         'domain=cognitive?':raw_vars_cognitive_map.get(var, np.nan),\n",
    "#         'replicating know human bias?':raw_vars_known_map.get(var, np.nan),\n",
    "#         'preference measure?':raw_vars_pref_map.get(var, np.nan),\n",
    "#         'stimuli dependent?':raw_vars_stim_map.get(var, np.nan),\n",
    "#         'sample size':        n\n",
    "#     })\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "#####participant-level data:\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "#########################unique to this study:\n",
    "# turn the index back into a column so melt can see it\n",
    "df_human.reset_index(inplace=True)\n",
    "df_twin.reset_index(inplace=True)\n",
    "###################\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "\n",
    "print(df_long.head())\n",
    "# save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6df31645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percent_correct humans: 51.75\n",
      "Standard deviation of percent_correct humans: 20.869267255691412\n",
      "Average percent_correct twis: 99.875\n",
      "Standard deviation of percent_correct twins: 1.8615209838073572\n",
      "Paired t-test on accuracy: t(399) = -45.888, p = 0.000  (n=400)\n",
      "Average fairness_average humans: 3.280416666666667\n",
      "Standard deviation of fairness_average humans: 1.239904301761693\n",
      "Average fairness_average twins: 3.2716666666666665\n",
      "Standard deviation of fairness_average twins: 0.6247940345671873\n",
      "Paired t-test on fairness: t(399) = 0.144, p = 0.886  (n=400)\n"
     ]
    }
   ],
   "source": [
    "# replicate pre-registered analysis\n",
    "\n",
    "###################################correct answers:\n",
    "average_total_correct = df_human[\"percent_correct\"].mean()\n",
    "print(f\"Average percent_correct humans: {average_total_correct}\")\n",
    "std_total_correct = df_human[\"percent_correct\"].std()\n",
    "print(f\"Standard deviation of percent_correct humans: {std_total_correct}\")\n",
    "\n",
    "average_total_correct = df_twin[\"percent_correct\"].mean()\n",
    "print(f\"Average percent_correct twis: {average_total_correct}\")\n",
    "std_total_correct = df_twin[\"percent_correct\"].std()\n",
    "print(f\"Standard deviation of percent_correct twins: {std_total_correct}\")\n",
    "\n",
    "acc_h = (df_human[\"percent_correct\"]).dropna()\n",
    "acc_t = (df_twin[\"percent_correct\"]).dropna()\n",
    "# align on TWIN_ID if you have them as index, otherwise just drop mismatches:\n",
    "paired_acc = pd.concat([acc_h, acc_t], axis=1, join=\"inner\").dropna()\n",
    "n_acc = len(paired_acc)\n",
    "t_acc, p_acc = ttest_rel(paired_acc.iloc[:, 0], paired_acc.iloc[:, 1])\n",
    "df_acc = n_acc - 1\n",
    "print(f\"Paired t-test on accuracy: t({df_acc}) = {t_acc:.3f}, p = {p_acc:.3f}  (n={n_acc})\")\n",
    "##############################\n",
    "\n",
    "\n",
    "################################fairness:\n",
    "average_fairness_average = df_human[\"fairness_average\"].mean()\n",
    "print(f\"Average fairness_average humans: {average_fairness_average}\")\n",
    "std_fairness = df_human[\"fairness_average\"].std()\n",
    "print(f\"Standard deviation of fairness_average humans: {std_fairness}\")\n",
    "\n",
    "average_fairness_average = df_twin[\"fairness_average\"].mean()\n",
    "print(f\"Average fairness_average twins: {average_fairness_average}\")\n",
    "std_fairness = df_twin[\"fairness_average\"].std()\n",
    "print(f\"Standard deviation of fairness_average twins: {std_fairness}\")\n",
    "\n",
    "fair_h = df_human[\"fairness_average\"].dropna()\n",
    "fair_t = df_twin[\"fairness_average\"].dropna()\n",
    "paired_fair = pd.concat([fair_h, fair_t], axis=1, join=\"inner\").dropna()\n",
    "n_fair = len(paired_fair)\n",
    "t_fair, p_fair = ttest_rel(paired_fair.iloc[:, 0], paired_fair.iloc[:, 1])\n",
    "df_fair = n_fair - 1\n",
    "print(f\"Paired t-test on fairness: t({df_fair}) = {t_fair:.3f}, p = {p_fair:.3f}  (n={n_fair})\")\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a164f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            reg_support   R-squared:                       0.322\n",
      "Model:                            OLS   Adj. R-squared:                  0.320\n",
      "Method:                 Least Squares   F-statistic:                     126.2\n",
      "Date:                Mon, 14 Jul 2025   Prob (F-statistic):           6.94e-67\n",
      "Time:                        15:49:21   Log-Likelihood:                -1349.9\n",
      "No. Observations:                 800   AIC:                             2708.\n",
      "Df Residuals:                     796   BIC:                             2726.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               5.0275      0.066     76.688      0.000       4.899       5.156\n",
      "source[T.AI]            0.4350      0.093      4.692      0.000       0.253       0.617\n",
      "ideo_c                 -0.4369      0.056     -7.782      0.000      -0.547      -0.327\n",
      "ideo_c:source[T.AI]    -0.5292      0.079     -6.665      0.000      -0.685      -0.373\n",
      "==============================================================================\n",
      "Omnibus:                       31.192   Durbin-Watson:                   2.012\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               33.983\n",
      "Skew:                          -0.503   Prob(JB):                     4.18e-08\n",
      "Kurtosis:                       3.094   Cond. No.                         3.06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# regression on political views:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "df_wave1 = pd.read_csv(\"Data_Politics.csv\", header=0)\n",
    "# Reverse‐code Q22\n",
    "pol_map = {\n",
    "    \"Very liberal\": 1,\n",
    "    \"Liberal\": 2,\n",
    "    \"Moderate\": 3,\n",
    "    \"Conservative\": 4,\n",
    "    \"Very conservative\": 5,\n",
    "}\n",
    "# apply it to create a numeric column\n",
    "df_wave1[\"political_view\"] = df_wave1[\"Politics\"].map(pol_map)\n",
    "df_human = df_human.drop(columns=\"political_view\", errors=\"ignore\")\n",
    "# Merge that back into df_human by TWIN_ID\n",
    "df_human = df_human.merge(df_wave1[[\"TWIN_ID\", \"political_view\"]], on=\"TWIN_ID\", how=\"left\")\n",
    "df_twin = df_twin.drop(columns=\"political_view\", errors=\"ignore\")\n",
    "# Merge that back into df_human by TWIN_ID\n",
    "df_twin = df_twin.merge(df_wave1[[\"TWIN_ID\", \"political_view\"]], on=\"TWIN_ID\", how=\"left\")\n",
    "\n",
    "# 1) Tag and stack\n",
    "df_h = df_human.copy()\n",
    "df_h[\"source\"] = \"Human\"\n",
    "df_t = df_twin.copy()\n",
    "df_t[\"source\"] = \"AI\"\n",
    "\n",
    "df_all = pd.concat([df_h, df_t], axis=0, ignore_index=True)\n",
    "\n",
    "df_all[\"source\"] = pd.Categorical(df_all[\"source\"], categories=[\"Human\", \"AI\"])\n",
    "\n",
    "# 2) Mean‐center ideology\n",
    "df_all[\"ideo_c\"] = df_all[\"political_view\"] - df_all[\"political_view\"].mean()\n",
    "\n",
    "# 3) Fit the regression:\n",
    "#    reg_support ~ ideo_c + source + ideo_c:source\n",
    "#    statsmodels will dummy‐code `source` for you\n",
    "model = smf.ols(\"reg_support ~ ideo_c * source\", data=df_all).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bf027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
