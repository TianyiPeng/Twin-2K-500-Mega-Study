{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0792ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ot2107\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ot2107\\appdata\\local\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "      study name persona specification variable name  \\\n",
      "0  Story Beliefs       default persona    Achap1_val   \n",
      "1  Story Beliefs       default persona    Achap1_aro   \n",
      "2  Story Beliefs       default persona    Achap2_val   \n",
      "3  Story Beliefs       default persona    Achap2_aro   \n",
      "4  Story Beliefs       default persona    Bchap1_val   \n",
      "5  Story Beliefs       default persona    Bchap1_aro   \n",
      "6  Story Beliefs       default persona    Bchap2_val   \n",
      "7  Story Beliefs       default persona    Bchap2_aro   \n",
      "\n",
      "   correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                           0.583898              0.495934   \n",
      "1                                           0.422336              0.314989   \n",
      "2                                           0.470576              0.368211   \n",
      "3                                           0.248305              0.128435   \n",
      "4                                           0.561564              0.470457   \n",
      "5                                           0.446906              0.342013   \n",
      "6                                           0.578781              0.490084   \n",
      "7                                           0.348248              0.234543   \n",
      "\n",
      "   CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0  0.660000                                          10.525291        \n",
      "1  0.519014                                           7.094981        \n",
      "2  0.561669                                           8.044240        \n",
      "3  0.361025                                           3.993787        \n",
      "4  0.640825                                          10.001801        \n",
      "5  0.540800                                           7.572085        \n",
      "6  0.655616                                          10.403587        \n",
      "7  0.452554                                           5.723602        \n",
      "\n",
      "   accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                 0.853635    2.976096   3.005378  ...   \n",
      "1                                 0.843202    3.519661   3.507649  ...   \n",
      "2                                 0.840558    3.108008   3.140916  ...   \n",
      "3                                 0.838659    3.590414   3.783506  ...   \n",
      "4                                 0.843227    3.018008   3.222948  ...   \n",
      "5                                 0.838277    3.549841   3.661992  ...   \n",
      "6                                 0.850757    3.176773   3.202510  ...   \n",
      "7                                 0.840906    3.743785   3.898805  ...   \n",
      "\n",
      "   effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                         NaN                        NaN               0   \n",
      "1                         NaN                        NaN               0   \n",
      "2                         NaN                        NaN               0   \n",
      "3                         NaN                        NaN               0   \n",
      "4                         NaN                        NaN               0   \n",
      "5                         NaN                        NaN               0   \n",
      "6                         NaN                        NaN               0   \n",
      "7                         NaN                        NaN               0   \n",
      "\n",
      "   domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                  0                             0                    0   \n",
      "1                  0                             0                    0   \n",
      "2                  0                             0                    0   \n",
      "3                  0                             0                    0   \n",
      "4                  0                             0                    0   \n",
      "5                  0                             0                    0   \n",
      "6                  0                             0                    0   \n",
      "7                  0                             0                    0   \n",
      "\n",
      "   stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                   1                    0                    0          251  \n",
      "1                   1                    0                    0          251  \n",
      "2                   1                    0                    0          251  \n",
      "3                   1                    0                    0          251  \n",
      "4                   1                    0                    0          251  \n",
      "5                   1                    0                    0          251  \n",
      "6                   1                    0                    0          251  \n",
      "7                   1                    0                    0          251  \n",
      "\n",
      "[8 rows x 26 columns]\n",
      "   TWIN_ID variable_name  value respondent_type     study_name  \\\n",
      "0        1    Achap1_val    3.7           human  Story Beliefs   \n",
      "1        3    Achap1_val    3.0           human  Story Beliefs   \n",
      "2        4    Achap1_val    3.2           human  Story Beliefs   \n",
      "3        5    Achap1_val    2.1           human  Story Beliefs   \n",
      "4       11    Achap1_val    3.0           human  Story Beliefs   \n",
      "\n",
      "  specification_name  \n",
      "0    default persona  \n",
      "1    default persona  \n",
      "2    default persona  \n",
      "3    default persona  \n",
      "4    default persona  \n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_min\"] = min_map[var]\n",
      "C:\\Users\\ot2107\\AppData\\Local\\Temp\\ipykernel_344\\847531666.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{var}_max\"] = max_map[var]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "!pip install --upgrade gensim\n",
    "import re\n",
    "from itertools import permutations\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine, euclidean, pdist\n",
    "\n",
    "# Load data\n",
    "study_name = \"Story Beliefs\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data values anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized.csv\"\n",
    "df_human = pd.read_csv(human_file, header=0, skiprows=[1, 2])\n",
    "df_twin = pd.read_csv(twin_file, header=0, skiprows=[1, 2])\n",
    "\n",
    "\n",
    "#########################\n",
    "# create new relevant columns:\n",
    "\n",
    "# define your weights once\n",
    "weights = np.arange(1, 6)\n",
    "\n",
    "# for each dataframe...\n",
    "for df in (df_human, df_twin):\n",
    "    # loop over A vs B, chap1 vs chap2, val vs aro\n",
    "    for prefix in [\"A\", \"B\"]:\n",
    "        for chap in [1, 2]:\n",
    "            for metric in [\"val\", \"aro\"]:\n",
    "                # build the five source columns\n",
    "                cols = [f\"{prefix}chap{chap}_{metric}_{i}\" for i in range(1, 6)]\n",
    "\n",
    "                # new column name, e.g. \"Achap1_val\" or \"Bchap2_aro\"\n",
    "                newcol = f\"{prefix}chap{chap}_{metric}\"\n",
    "\n",
    "                # compute denominator (sum of counts)\n",
    "                denom = df[cols].sum(axis=1)\n",
    "\n",
    "                # weighted sum divided by total count\n",
    "                df[newcol] = df[cols].dot(weights) / denom\n",
    "\n",
    "                # optional: set to NaN where there were no counts\n",
    "                df.loc[denom == 0, newcol] = np.nan\n",
    "###############################\n",
    "\n",
    "\n",
    "human_file = f\"{study_name} human data values anonymized processed.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized processed.csv\"\n",
    "df_twin.to_csv(twin_file, index=False)\n",
    "df_human.to_csv(human_file, index=False)\n",
    "\n",
    "df_human = df_human.set_index(\"TWIN_ID\")\n",
    "df_twin = df_twin.set_index(\"TWIN_ID\")\n",
    "\n",
    "\n",
    "##################now proceed with standard meta-analysis steps\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "# raw responses:\n",
    "# all the “raw” columns\n",
    "# raw_vars = [\n",
    "#     f\"{prefix}chap{chap}_{metric}_{i}\"\n",
    "#     for prefix in [\"A\", \"B\"]\n",
    "#     for chap   in [1, 2]\n",
    "#     for metric in [\"val\", \"aro\"]\n",
    "#     for i      in range(1, 6)\n",
    "# ]\n",
    "# # check\n",
    "# print(len(raw_vars), \"columns:\", raw_vars)\n",
    "# raw_vars_min = [0]*40\n",
    "# raw_vars_max = [100]*40\n",
    "# #raw responses: domain=social?\n",
    "# raw_vars_social=[0]*40\n",
    "# raw_vars_social_map = dict(zip(raw_vars, raw_vars_social))\n",
    "# #raw responses: domain=cognitive?\n",
    "# raw_vars_cognitive=[0]*40\n",
    "# raw_vars_cognitive_map = dict(zip(raw_vars, raw_vars_cognitive))\n",
    "# #raw responses: replicating know human bias?\n",
    "# raw_vars_known=[0]*40\n",
    "# raw_vars_known_map = dict(zip(raw_vars, raw_vars_known))\n",
    "# #raw responses: preference measure?\n",
    "# raw_vars_pref=[0]*40\n",
    "# raw_vars_pref_map = dict(zip(raw_vars, raw_vars_pref))\n",
    "# #raw responses: stimuli dependent?\n",
    "# raw_vars_stim=[1]*40\n",
    "# raw_vars_stim_map = dict(zip(raw_vars, raw_vars_stim))\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [\n",
    "    f\"{prefix}chap{chap}_{metric}\"\n",
    "    for prefix in [\"A\", \"B\"]\n",
    "    for chap in [1, 2]\n",
    "    for metric in [\"val\", \"aro\"]\n",
    "]\n",
    "DV_vars_min = [1] * 8\n",
    "DV_vars_max = [5] * 8\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [0] * 8\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [0] * 8\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0] * 8\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [0] * 8\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [1] * 8\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [0] * 8\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0] * 8\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "# Fix dtypes\n",
    "# for var in raw_vars + DV_vars:\n",
    "for var in DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "# for var in raw_vars:\n",
    "#     col_h = f\"{var}_human\"\n",
    "#     col_t = f\"{var}_twin\"\n",
    "#     min_col = f\"{var}_min\"\n",
    "#     max_col = f\"{var}_max\"\n",
    "#     if cond_exists:\n",
    "#         cols = [col_h, col_t, cond_h, cond_t,min_col,max_col]\n",
    "#     else:\n",
    "#         cols = [col_h, col_t,min_col,max_col]\n",
    "#     pair = (\n",
    "#     df[cols]\n",
    "#       .dropna(subset=[col_h, col_t])\n",
    "#     )\n",
    "#     min_val = pair[min_col].iloc[0]\n",
    "#     max_val = pair[max_col].iloc[0]\n",
    "#     n    = len(pair)\n",
    "#     if n >= 4:\n",
    "#         r, _    = pearsonr(pair[col_h], pair[col_t])\n",
    "#         z_f     = np.arctanh(r)\n",
    "#         se      = 1 / np.sqrt(n - 3)\n",
    "#         z_crit  = norm.ppf(0.975)\n",
    "#         lo_z, hi_z = z_f - z_crit*se, z_f + z_crit*se\n",
    "#         lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "#         z_score    = z_f / se\n",
    "#         # Accuracy = mean absolute diff / range\n",
    "#         if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "#             accuracy = np.nan\n",
    "#         else:\n",
    "#             # compute mean absolute difference\n",
    "#             abs_diff      = np.abs(pair[col_h] - pair[col_t])\n",
    "#             mean_abs_diff = abs_diff.mean()\n",
    "#             accuracy      = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "#         mean_h = pair[col_h].mean()\n",
    "#         mean_t = pair[col_t].mean()\n",
    "\n",
    "#         # Paired t‐test\n",
    "#         t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "#         std_h = pair[col_h].std(ddof=1)\n",
    "#         std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "#          # F‐test for equal variances\n",
    "#         df1 = df2 = n - 1\n",
    "#         f_stat = (std_h**2 / std_t**2) if std_t>0 else np.nan\n",
    "\n",
    "#         # two‐tailed p‐value:\n",
    "#         if not np.isnan(f_stat):\n",
    "#             p_f = 2 * min(f.cdf(f_stat, df1, df2),\n",
    "#                           1 - f.cdf(f_stat, df1, df2))\n",
    "#         else:\n",
    "#             p_f = np.nan\n",
    "\n",
    "#         # Effect sizes (Cohen's d) across conditions\n",
    "#         #    For humans:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_h = pair[cond_h].unique()\n",
    "#             if len(levels_h) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_h]==levels_h[0], col_h]\n",
    "#                 g2 = pair.loc[pair[cond_h]==levels_h[1], col_h]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 # pooled sd\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_human = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_human = np.nan\n",
    "#         else:\n",
    "#             d_human = np.nan\n",
    "\n",
    "#         #    For twins:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_t = pair[cond_t].unique()\n",
    "#             if cond_exists and len(levels_t) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_t]==levels_t[0], col_t]\n",
    "#                 g2 = pair.loc[pair[cond_t]==levels_t[1], col_t]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_twin = np.nan\n",
    "#         else:\n",
    "#             d_twin = np.nan\n",
    "#     else:\n",
    "#         r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = f_stat = p_f = np.nan\n",
    "#         d_human = d_twin = np.nan\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         'study name': study_name,\n",
    "#         'variable name': var,\n",
    "#         'variable type (raw response/DV)':     'raw',\n",
    "#         'correlation between the responses from humans vs. their twins':        r,\n",
    "#         'CI_lower': lo_r,\n",
    "#         'CI_upper': hi_r,\n",
    "#         'z-score for correlation between humans vs. their twins':  z_score,\n",
    "#         'accuracy between humans vs. their twins': accuracy,\n",
    "#         'mean_human': mean_h,\n",
    "#         'mean_twin': mean_t,\n",
    "#         'paired t-test t-stat': t_stat,\n",
    "#         'paired t-test p-value': p_val,\n",
    "#         'std_human': std_h,\n",
    "#         'std_twin': std_t,\n",
    "#         'variance test F-stat': f_stat,\n",
    "#         'variance test p-value': p_f,\n",
    "#         'effect size based on human': d_human,\n",
    "#         'effect size based on twin': d_twin,\n",
    "#         'domain=social?':raw_vars_social_map.get(var, np.nan),\n",
    "#         'domain=cognitive?':raw_vars_cognitive_map.get(var, np.nan),\n",
    "#         'replicating know human bias?':raw_vars_known_map.get(var, np.nan),\n",
    "#         'preference measure?':raw_vars_pref_map.get(var, np.nan),\n",
    "#         'stimuli dependent?':raw_vars_stim_map.get(var, np.nan),\n",
    "#         'sample size':        n\n",
    "#     })\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "#####participant-level data:\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "#########################unique to this study:\n",
    "# turn the index back into a column so melt can see it\n",
    "df_human.reset_index(inplace=True)\n",
    "df_twin.reset_index(inplace=True)\n",
    "###################\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "\n",
    "print(df_long.head())\n",
    "# save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96d8fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dimension         r          t    df       p-value  CI Lower  CI Upper\n",
      "0   Valence  0.546342  20.648183  1002  3.356481e-79  0.501422  0.588325\n",
      "1   Arousal  0.384209  13.172995  1002  1.154545e-36  0.330189  0.435721\n"
     ]
    }
   ],
   "source": [
    "# replicate pre-registered analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, pearsonr\n",
    "\n",
    "# 1) Re-load and process your data exactly as above\n",
    "study_name = \"Story Beliefs\"\n",
    "df_human = pd.read_csv(\n",
    "    f\"{study_name} human data values anonymized processed.csv\", header=0\n",
    ").set_index(\"TWIN_ID\")\n",
    "df_twin = pd.read_csv(\n",
    "    f\"{study_name} twins data values anonymized processed.csv\", header=0\n",
    ").set_index(\"TWIN_ID\")\n",
    "\n",
    "\n",
    "# 2) Melt function\n",
    "def melt_story_data(df, source_label):\n",
    "    pieces = []\n",
    "    for prefix in [\"A\", \"B\"]:\n",
    "        for chap in [1, 2]:\n",
    "            val_col = f\"{prefix}chap{chap}_val\"\n",
    "            aro_col = f\"{prefix}chap{chap}_aro\"\n",
    "            if val_col in df.columns and aro_col in df.columns:\n",
    "                tmp = df[[val_col, aro_col]].copy()\n",
    "                tmp.columns = [\"expected_valence_next\", \"expected_arousal_next\"]\n",
    "                tmp = tmp.reset_index().rename(columns={\"index\": \"TWIN_ID\"})\n",
    "                tmp[\"story_chapter_survey\"] = f\"{prefix}chap{chap}\"\n",
    "                tmp[\"source\"] = source_label\n",
    "                pieces.append(tmp)\n",
    "    return pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "\n",
    "# 3) Melt both\n",
    "human_melted = melt_story_data(df_human, \"human\")\n",
    "twin_melted = melt_story_data(df_twin, \"twin\")\n",
    "combined_melted = pd.concat([human_melted, twin_melted], ignore_index=True)\n",
    "\n",
    "# 4) Pivot wide to align human/twin for each chapter\n",
    "corr_wide = combined_melted.pivot_table(\n",
    "    index=[\"TWIN_ID\", \"story_chapter_survey\"],\n",
    "    columns=\"source\",\n",
    "    values=[\"expected_valence_next\", \"expected_arousal_next\"],\n",
    ").dropna()\n",
    "corr_wide.columns = [f\"{metric}_{src}\" for metric, src in corr_wide.columns]\n",
    "\n",
    "# 5) Compute correlation stats\n",
    "n = corr_wide.shape[0]\n",
    "df_stat = n - 2\n",
    "\n",
    "\n",
    "def corr_stats(x, y):\n",
    "    r, p = pearsonr(x, y)\n",
    "    t = r * np.sqrt(df_stat / (1 - r**2))\n",
    "    se_z = 1 / np.sqrt(n - 3)\n",
    "    z = np.arctanh(r)\n",
    "    z_crit = norm.ppf(0.975)\n",
    "    lo_z, hi_z = z - z_crit * se_z, z + z_crit * se_z\n",
    "    lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "    return r, t, df_stat, p, lo_r, hi_r\n",
    "\n",
    "\n",
    "r_v, t_v, df_v, p_v, lo_v, hi_v = corr_stats(\n",
    "    corr_wide[\"expected_valence_next_human\"], corr_wide[\"expected_valence_next_twin\"]\n",
    ")\n",
    "r_a, t_a, df_a, p_a, lo_a, hi_a = corr_stats(\n",
    "    corr_wide[\"expected_arousal_next_human\"], corr_wide[\"expected_arousal_next_twin\"]\n",
    ")\n",
    "\n",
    "# 6) Display results\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\": [\"Valence\", \"Arousal\"],\n",
    "        \"r\": [r_v, r_a],\n",
    "        \"t\": [t_v, t_a],\n",
    "        \"df\": [df_v, df_a],\n",
    "        \"p-value\": [p_v, p_a],\n",
    "        \"CI Lower\": [lo_v, lo_a],\n",
    "        \"CI Upper\": [hi_v, hi_a],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8e1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in human_melted: 1004\n",
      "Number of rows in twin_melted: 1004\n",
      "Number of rows in combined_melted: 2008\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(human_melted)\n",
    "print(f\"Number of rows in human_melted: {n_rows}\")\n",
    "\n",
    "n_rows = len(twin_melted)\n",
    "print(f\"Number of rows in twin_melted: {n_rows}\")\n",
    "\n",
    "n_rows = len(combined_melted)\n",
    "print(f\"Number of rows in combined_melted: {n_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff935c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
