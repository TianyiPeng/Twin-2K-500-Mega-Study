{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab47485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All conditions agree: True\n",
      "Mismatched IDs and their human vs twin conditions:\n",
      "         study name persona specification variable name  \\\n",
      "0   idea evaluation       default persona       rating1   \n",
      "1   idea evaluation       default persona       rating2   \n",
      "2   idea evaluation       default persona       rating3   \n",
      "3   idea evaluation       default persona       rating4   \n",
      "4   idea evaluation       default persona       rating5   \n",
      "5   idea evaluation       default persona       rating6   \n",
      "6   idea evaluation       default persona       rating7   \n",
      "7   idea evaluation       default persona       rating8   \n",
      "8   idea evaluation       default persona       rating9   \n",
      "9   idea evaluation       default persona      rating10   \n",
      "10  idea evaluation       default persona      rating11   \n",
      "11  idea evaluation       default persona      rating12   \n",
      "12  idea evaluation       default persona      rating13   \n",
      "13  idea evaluation       default persona      rating14   \n",
      "14  idea evaluation       default persona      rating15   \n",
      "15  idea evaluation       default persona      rating16   \n",
      "16  idea evaluation       default persona      rating17   \n",
      "17  idea evaluation       default persona      rating18   \n",
      "18  idea evaluation       default persona      rating19   \n",
      "19  idea evaluation       default persona      rating20   \n",
      "\n",
      "    correlation between the responses from humans vs. their twins  CI_lower  \\\n",
      "0                                            0.009936             -0.047304   \n",
      "1                                            0.080166              0.023058   \n",
      "2                                            0.079482              0.022370   \n",
      "3                                            0.016271             -0.040981   \n",
      "4                                            0.016552             -0.040700   \n",
      "5                                            0.029531             -0.027729   \n",
      "6                                            0.035347             -0.021911   \n",
      "7                                            0.095892              0.038892   \n",
      "8                                            0.047310             -0.009930   \n",
      "9                                            0.018922             -0.038332   \n",
      "10                                           0.046060             -0.011182   \n",
      "11                                           0.002210             -0.055010   \n",
      "12                                           0.076199              0.019069   \n",
      "13                                           0.050851             -0.006381   \n",
      "14                                           0.049002             -0.008234   \n",
      "15                                          -0.025347             -0.082440   \n",
      "16                                          -0.013026             -0.070187   \n",
      "17                                           0.023224             -0.034034   \n",
      "18                                           0.033912             -0.023346   \n",
      "19                                          -0.004330             -0.061528   \n",
      "\n",
      "    CI_upper  z-score for correlation between humans vs. their twins  \\\n",
      "0   0.067111                                           0.340020        \n",
      "1   0.136751                                           2.749153        \n",
      "2   0.136076                                           2.725607        \n",
      "3   0.073415                                           0.556828        \n",
      "4   0.073695                                           0.566460        \n",
      "5   0.086598                                           1.010839        \n",
      "6   0.092373                                           1.210059        \n",
      "7   0.152270                                           3.291529        \n",
      "8   0.104241                                           1.620164        \n",
      "9   0.076053                                           0.647599        \n",
      "10  0.103002                                           1.577285        \n",
      "11  0.059416                                           0.075630        \n",
      "12  0.132833                                           2.612577        \n",
      "13  0.107750                                           1.741614        \n",
      "14  0.105918                                           1.678179        \n",
      "15  0.031913                                          -0.867546        \n",
      "16  0.044220                                          -0.445781        \n",
      "17  0.080330                                           0.794859        \n",
      "18  0.090949                                           1.160916        \n",
      "19  0.052896                                          -0.148181        \n",
      "\n",
      "    accuracy between humans vs. their twins  mean_human  mean_twin  ...  \\\n",
      "0                                  0.716354    3.275128   3.210392  ...   \n",
      "1                                  0.730622    3.245315   3.259796  ...   \n",
      "2                                  0.730835    3.175468   3.272572  ...   \n",
      "3                                  0.723595    3.210392   3.237649  ...   \n",
      "4                                  0.726576    3.247871   3.247871  ...   \n",
      "5                                  0.723595    3.211244   3.257240  ...   \n",
      "6                                  0.718058    3.219761   3.229983  ...   \n",
      "7                                  0.724233    3.203578   3.226576  ...   \n",
      "8                                  0.727215    3.297274   3.218058  ...   \n",
      "9                                  0.720826    3.247871   3.221465  ...   \n",
      "10                                 0.731261    3.275128   3.273424  ...   \n",
      "11                                 0.720400    3.271721   3.236797  ...   \n",
      "12                                 0.725298    3.279387   3.212947  ...   \n",
      "13                                 0.727428    3.203578   3.230835  ...   \n",
      "14                                 0.714012    3.218910   3.289608  ...   \n",
      "15                                 0.717419    3.271721   3.252129  ...   \n",
      "16                                 0.709327    3.247019   3.283646  ...   \n",
      "17                                 0.710818    3.245315   3.223169  ...   \n",
      "18                                 0.724872    3.227428   3.234242  ...   \n",
      "19                                 0.718697    3.316865   3.252981  ...   \n",
      "\n",
      "    effect size based on human  effect size based on twin  domain=social?  \\\n",
      "0                          NaN                        NaN               1   \n",
      "1                          NaN                        NaN               1   \n",
      "2                          NaN                        NaN               1   \n",
      "3                          NaN                        NaN               1   \n",
      "4                          NaN                        NaN               1   \n",
      "5                          NaN                        NaN               1   \n",
      "6                          NaN                        NaN               1   \n",
      "7                          NaN                        NaN               1   \n",
      "8                          NaN                        NaN               1   \n",
      "9                          NaN                        NaN               1   \n",
      "10                         NaN                        NaN               1   \n",
      "11                         NaN                        NaN               1   \n",
      "12                         NaN                        NaN               1   \n",
      "13                         NaN                        NaN               1   \n",
      "14                         NaN                        NaN               1   \n",
      "15                         NaN                        NaN               1   \n",
      "16                         NaN                        NaN               1   \n",
      "17                         NaN                        NaN               1   \n",
      "18                         NaN                        NaN               1   \n",
      "19                         NaN                        NaN               1   \n",
      "\n",
      "    domain=cognitive?  replicating know human bias?  preference measure?  \\\n",
      "0                   0                             0                    1   \n",
      "1                   0                             0                    1   \n",
      "2                   0                             0                    1   \n",
      "3                   0                             0                    1   \n",
      "4                   0                             0                    1   \n",
      "5                   0                             0                    1   \n",
      "6                   0                             0                    1   \n",
      "7                   0                             0                    1   \n",
      "8                   0                             0                    1   \n",
      "9                   0                             0                    1   \n",
      "10                  0                             0                    1   \n",
      "11                  0                             0                    1   \n",
      "12                  0                             0                    1   \n",
      "13                  0                             0                    1   \n",
      "14                  0                             0                    1   \n",
      "15                  0                             0                    1   \n",
      "16                  0                             0                    1   \n",
      "17                  0                             0                    1   \n",
      "18                  0                             0                    1   \n",
      "19                  0                             0                    1   \n",
      "\n",
      "    stimuli dependent?  knowledge question?  political question?  sample size  \n",
      "0                    1                    0                    0         1174  \n",
      "1                    1                    0                    0         1174  \n",
      "2                    1                    0                    0         1174  \n",
      "3                    1                    0                    0         1174  \n",
      "4                    1                    0                    0         1174  \n",
      "5                    1                    0                    0         1174  \n",
      "6                    1                    0                    0         1174  \n",
      "7                    1                    0                    0         1174  \n",
      "8                    1                    0                    0         1174  \n",
      "9                    1                    0                    0         1174  \n",
      "10                   1                    0                    0         1174  \n",
      "11                   1                    0                    0         1174  \n",
      "12                   1                    0                    0         1174  \n",
      "13                   1                    0                    0         1174  \n",
      "14                   1                    0                    0         1174  \n",
      "15                   1                    0                    0         1174  \n",
      "16                   1                    0                    0         1174  \n",
      "17                   1                    0                    0         1174  \n",
      "18                   1                    0                    0         1174  \n",
      "19                   1                    0                    0         1174  \n",
      "\n",
      "[20 rows x 26 columns]\n",
      "   TWIN_ID variable_name  value respondent_type       study_name  \\\n",
      "0        1       rating1    2.0           human  idea evaluation   \n",
      "1        2       rating1    4.0           human  idea evaluation   \n",
      "2        5       rating1    3.0           human  idea evaluation   \n",
      "3       10       rating1    2.0           human  idea evaluation   \n",
      "4       14       rating1    3.0           human  idea evaluation   \n",
      "\n",
      "  specification_name  \n",
      "0    default persona  \n",
      "1    default persona  \n",
      "2    default persona  \n",
      "3    default persona  \n",
      "4    default persona  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f, norm, pearsonr, ttest_rel\n",
    "\n",
    "# Load data\n",
    "study_name = \"idea evaluation\"\n",
    "specification_name = \"default persona\"\n",
    "human_file = f\"{study_name} human data values anonymized.csv\"\n",
    "twin_file = f\"{study_name} twins data values anonymized.csv\"\n",
    "df_human = pd.read_csv(human_file, header=0, skiprows=[1, 2])\n",
    "df_twin = pd.read_csv(twin_file, header=0, skiprows=[1, 2])\n",
    "\n",
    "# create new relevant columns:\n",
    "df_human = df_human.set_index(\"TWIN_ID\")\n",
    "df_twin = df_twin.set_index(\"TWIN_ID\")\n",
    "# define the six sets of columns\n",
    "baseline_human = [f\"{i}_Q11\" for i in range(1, 201)]\n",
    "baseline_ai = [f\"{i}_Q23\" for i in range(1, 201)]\n",
    "partial_human = [f\"{i}_Q13\" for i in range(1, 201)]\n",
    "partial_ai = [f\"{i}_Q24\" for i in range(1, 201)]\n",
    "full_human = [f\"{i}_Q14\" for i in range(1, 201)]\n",
    "full_ai = [f\"{i}_Q25\" for i in range(1, 201)]\n",
    "# assign the 'condition' based on which block has any non‐missing\n",
    "df_human[\"condition\"] = np.where(\n",
    "    df_human[baseline_human].notna().any(axis=1),\n",
    "    \"human ideas - baseline\",\n",
    "    np.where(\n",
    "        df_human[baseline_ai].notna().any(axis=1),\n",
    "        \"AI ideas - baseline\",\n",
    "        np.where(\n",
    "            df_human[partial_human].notna().any(axis=1),\n",
    "            \"human ideas - partial\",\n",
    "            np.where(\n",
    "                df_human[partial_ai].notna().any(axis=1),\n",
    "                \"AI ideas - partial\",\n",
    "                np.where(\n",
    "                    df_human[full_human].notna().any(axis=1),\n",
    "                    \"human ideas - full\",\n",
    "                    np.where(df_human[full_ai].notna().any(axis=1), \"AI ideas - full\", np.nan),\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "df_twin[\"condition\"] = np.where(\n",
    "    df_twin[baseline_human].notna().any(axis=1),\n",
    "    \"human ideas - baseline\",\n",
    "    np.where(\n",
    "        df_twin[baseline_ai].notna().any(axis=1),\n",
    "        \"AI ideas - baseline\",\n",
    "        np.where(\n",
    "            df_twin[partial_human].notna().any(axis=1),\n",
    "            \"human ideas - partial\",\n",
    "            np.where(\n",
    "                df_twin[partial_ai].notna().any(axis=1),\n",
    "                \"AI ideas - partial\",\n",
    "                np.where(\n",
    "                    df_twin[full_human].notna().any(axis=1),\n",
    "                    \"human ideas - full\",\n",
    "                    np.where(df_twin[full_ai].notna().any(axis=1), \"AI ideas - full\", np.nan),\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "# verify that condition assignment is the same for humans and twins:\n",
    "# find the intersection of IDs just to be safe\n",
    "common_ids = df_human.index.intersection(df_twin.index)\n",
    "# a boolean Series saying whether they match for each ID\n",
    "matches = df_human.loc[common_ids, \"condition\"].eq(df_twin.loc[common_ids, \"condition\"])\n",
    "# check if *all* match\n",
    "all_match = matches.all()\n",
    "print(\"All conditions agree:\", all_match)\n",
    "# if you want to see which ones don’t match:\n",
    "mismatches = matches[~matches]\n",
    "print(\"Mismatched IDs and their human vs twin conditions:\")\n",
    "for twin_id in mismatches.index:\n",
    "    print(\n",
    "        twin_id,\n",
    "        \"human →\",\n",
    "        df_human.at[twin_id, \"condition\"],\n",
    "        \"| twin →\",\n",
    "        df_twin.at[twin_id, \"condition\"],\n",
    "    )\n",
    "# count total non‐missing across *all* 6 blocks\n",
    "all_cols = baseline_human + baseline_ai + partial_human + partial_ai + full_human + full_ai\n",
    "df_human[\"nratings\"] = df_twin[all_cols].notna().sum(axis=1)\n",
    "df_twin[\"nratings\"] = df_twin[all_cols].notna().sum(axis=1)\n",
    "# pull out the 20 non‐missing values in order into rating1…rating20\n",
    "#    (we assume each row has exactly 20 non‐nulls; if not, extra will be NaN)\n",
    "rating_lists = df_human[all_cols].apply(lambda row: row.dropna().tolist(), axis=1)\n",
    "rating_df = pd.DataFrame(\n",
    "    rating_lists.tolist(), index=df_human.index, columns=[f\"rating{i + 1}\" for i in range(20)]\n",
    ")\n",
    "df_human = pd.concat([df_human, rating_df], axis=1)\n",
    "rating_lists = df_twin[all_cols].apply(lambda row: row.dropna().tolist(), axis=1)\n",
    "rating_df = pd.DataFrame(\n",
    "    rating_lists.tolist(), index=df_twin.index, columns=[f\"rating{i + 1}\" for i in range(20)]\n",
    ")\n",
    "df_twin = pd.concat([df_twin, rating_df], axis=1)\n",
    "# sanity‐check: nratings should be 20 everywhere\n",
    "assert (df_human[\"nratings\"] == 20).all(), \"Some rows do not have exactly 20 ratings!\"\n",
    "assert (df_twin[\"nratings\"] == 20).all(), \"Some rows do not have exactly 20 ratings!\"\n",
    "df_twin.to_csv(\"idea_evaluation_twins_processed.csv\", index=False)\n",
    "df_human.to_csv(\"idea_evaluation_human_processed.csv\", index=False)\n",
    "\n",
    "# define relevant columns:\n",
    "# condition variable names:\n",
    "condition_vars = [\"condition\"]\n",
    "# Check if we have a real condition var\n",
    "if condition_vars and condition_vars[0].strip():\n",
    "    cond = condition_vars[0]\n",
    "    cond_h = f\"{cond}_human\"\n",
    "    cond_t = f\"{cond}_twin\"\n",
    "    cond_exists = True\n",
    "else:\n",
    "    cond_exists = False\n",
    "\n",
    "# #raw responses:\n",
    "# raw_vars = [f\"rating{i}\" for i in range(1, 21)]\n",
    "# raw_vars_min = [1]*20\n",
    "# raw_vars_max = [5]*20\n",
    "# #raw responses: domain=social?\n",
    "# raw_vars_social=[1]*20\n",
    "# raw_vars_social_map = dict(zip(raw_vars, raw_vars_social))\n",
    "# #raw responses: domain=cognitive?\n",
    "# raw_vars_cognitive=[0]*20\n",
    "# raw_vars_cognitive_map = dict(zip(raw_vars, raw_vars_cognitive))\n",
    "# #raw responses: replicating know human bias?\n",
    "# raw_vars_known=[0]*20\n",
    "# raw_vars_known_map = dict(zip(raw_vars, raw_vars_known))\n",
    "# #raw responses: preference measure?\n",
    "# raw_vars_pref=[1]*20\n",
    "# raw_vars_pref_map = dict(zip(raw_vars, raw_vars_pref))\n",
    "# #raw responses: stimuli dependent?\n",
    "# raw_vars_stim=[1]*20\n",
    "# raw_vars_stim_map = dict(zip(raw_vars, raw_vars_stim))\n",
    "\n",
    "# DVs:\n",
    "DV_vars = [f\"rating{i}\" for i in range(1, 21)]\n",
    "DV_vars_min = [1] * 20\n",
    "DV_vars_max = [5] * 20\n",
    "# DVs: domain=social?\n",
    "DV_vars_social = [1] * 20\n",
    "DV_vars_social_map = dict(zip(DV_vars, DV_vars_social))\n",
    "# DVs: domain=cognitive?\n",
    "DV_vars_cognitive = [0] * 20\n",
    "DV_vars_cognitive_map = dict(zip(DV_vars, DV_vars_cognitive))\n",
    "# DVs: replicating know human bias?\n",
    "DV_vars_known = [0] * 20\n",
    "DV_vars_known_map = dict(zip(DV_vars, DV_vars_known))\n",
    "# DVs: preference measure?\n",
    "DV_vars_pref = [1] * 20\n",
    "DV_vars_pref_map = dict(zip(DV_vars, DV_vars_pref))\n",
    "# DVs: stimuli dependent?\n",
    "DV_vars_stim = [1] * 20\n",
    "DV_vars_stim_map = dict(zip(DV_vars, DV_vars_stim))\n",
    "# DVs: knowledge question?\n",
    "DV_vars_know = [0] * 20\n",
    "DV_vars_know_map = dict(zip(DV_vars, DV_vars_know))\n",
    "# DVs: political question?\n",
    "DV_vars_politics = [0] * 20\n",
    "DV_vars_politics_map = dict(zip(DV_vars, DV_vars_politics))\n",
    "\n",
    "# merging key\n",
    "merge_key = [\"TWIN_ID\"]\n",
    "\n",
    "# Merge on TWIN_ID\n",
    "df = pd.merge(df_human, df_twin, on=merge_key, suffixes=(\"_human\", \"_twin\"))\n",
    "\n",
    "# Fix dtypes\n",
    "# for var in raw_vars + DV_vars:\n",
    "for var in DV_vars:\n",
    "    df[f\"{var}_human\"] = pd.to_numeric(df[f\"{var}_human\"], errors=\"coerce\")\n",
    "    df[f\"{var}_twin\"] = pd.to_numeric(df[f\"{var}_twin\"], errors=\"coerce\")\n",
    "\n",
    "# build min/max maps from both raw_vars and DV_vars\n",
    "min_map = {v: mn for v, mn in zip(DV_vars, DV_vars_min)}\n",
    "# min_map = {v: mn for v, mn in zip(raw_vars,      raw_vars_min)}\n",
    "# min_map.update({v: mn for v, mn in zip(DV_vars,   DV_vars_min)})\n",
    "\n",
    "max_map = {v: mx for v, mx in zip(DV_vars, DV_vars_max)}\n",
    "# max_map = {v: mx for v, mx in zip(raw_vars,      raw_vars_max)}\n",
    "# max_map.update({v: mx for v, mx in zip(DV_vars,   DV_vars_max)})\n",
    "\n",
    "# now add _min and _max columns for every variable in the union\n",
    "for var in min_map:\n",
    "    df[f\"{var}_min\"] = min_map[var]\n",
    "    df[f\"{var}_max\"] = max_map[var]\n",
    "\n",
    "# Compute results\n",
    "results = []\n",
    "# for var in raw_vars:\n",
    "#     col_h = f\"{var}_human\"\n",
    "#     col_t = f\"{var}_twin\"\n",
    "#     min_col = f\"{var}_min\"\n",
    "#     max_col = f\"{var}_max\"\n",
    "#     if cond_exists:\n",
    "#         cols = [col_h, col_t, cond_h, cond_t,min_col,max_col]\n",
    "#     else:\n",
    "#         cols = [col_h, col_t,min_col,max_col]\n",
    "#     pair = (\n",
    "#     df[cols]\n",
    "#       .dropna(subset=[col_h, col_t])\n",
    "#     )\n",
    "#     min_val = pair[min_col].iloc[0]\n",
    "#     max_val = pair[max_col].iloc[0]\n",
    "#     n    = len(pair)\n",
    "#     if n >= 4:\n",
    "#         r, _    = pearsonr(pair[col_h], pair[col_t])\n",
    "#         z_f     = np.arctanh(r)\n",
    "#         se      = 1 / np.sqrt(n - 3)\n",
    "#         z_crit  = norm.ppf(0.975)\n",
    "#         lo_z, hi_z = z_f - z_crit*se, z_f + z_crit*se\n",
    "#         lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "#         z_score    = z_f / se\n",
    "#         # Accuracy = mean absolute diff / range\n",
    "#         if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "#             accuracy = np.nan\n",
    "#         else:\n",
    "#             # compute mean absolute difference\n",
    "#             abs_diff      = np.abs(pair[col_h] - pair[col_t])\n",
    "#             mean_abs_diff = abs_diff.mean()\n",
    "#             accuracy      = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "#         mean_h = pair[col_h].mean()\n",
    "#         mean_t = pair[col_t].mean()\n",
    "\n",
    "#         # Paired t‐test\n",
    "#         t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "#         std_h = pair[col_h].std(ddof=1)\n",
    "#         std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "#          # F‐test for equal variances\n",
    "#         df1 = df2 = n - 1\n",
    "#         f_stat = (std_h**2 / std_t**2) if std_t>0 else np.nan\n",
    "\n",
    "#         # two‐tailed p‐value:\n",
    "#         if not np.isnan(f_stat):\n",
    "#             p_f = 2 * min(f.cdf(f_stat, df1, df2),\n",
    "#                           1 - f.cdf(f_stat, df1, df2))\n",
    "#         else:\n",
    "#             p_f = np.nan\n",
    "\n",
    "#         # Effect sizes (Cohen's d) across conditions\n",
    "#         #    For humans:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_h = pair[cond_h].unique()\n",
    "#             if len(levels_h) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_h]==levels_h[0], col_h]\n",
    "#                 g2 = pair.loc[pair[cond_h]==levels_h[1], col_h]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 # pooled sd\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_human = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_human = np.nan\n",
    "#         else:\n",
    "#             d_human = np.nan\n",
    "\n",
    "#         #    For twins:\n",
    "#         if cond_exists and len(pair)>3:\n",
    "#             levels_t = pair[cond_t].unique()\n",
    "#             if cond_exists and len(levels_t) == 2:\n",
    "#                 g1 = pair.loc[pair[cond_t]==levels_t[0], col_t]\n",
    "#                 g2 = pair.loc[pair[cond_t]==levels_t[1], col_t]\n",
    "#                 n1, n2 = len(g1), len(g2)\n",
    "#                 s_pool = np.sqrt(((n1-1)*g1.var(ddof=1)+(n2-1)*g2.var(ddof=1)) / (n1+n2-2))\n",
    "#                 d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool>0 else np.nan\n",
    "#             else:\n",
    "#                 d_twin = np.nan\n",
    "#         else:\n",
    "#             d_twin = np.nan\n",
    "#     else:\n",
    "#         r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = f_stat = p_f = np.nan\n",
    "#         d_human = d_twin = np.nan\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         'study name': study_name,\n",
    "#         'variable name': var,\n",
    "#         'variable type (raw response/DV)':     'raw',\n",
    "#         'correlation between the responses from humans vs. their twins':        r,\n",
    "#         'CI_lower': lo_r,\n",
    "#         'CI_upper': hi_r,\n",
    "#         'z-score for correlation between humans vs. their twins':  z_score,\n",
    "#         'accuracy between humans vs. their twins': accuracy,\n",
    "#         'mean_human': mean_h,\n",
    "#         'mean_twin': mean_t,\n",
    "#         'paired t-test t-stat': t_stat,\n",
    "#         'paired t-test p-value': p_val,\n",
    "#         'std_human': std_h,\n",
    "#         'std_twin': std_t,\n",
    "#         'variance test F-stat': f_stat,\n",
    "#         'variance test p-value': p_f,\n",
    "#         'effect size based on human': d_human,\n",
    "#         'effect size based on twin': d_twin,\n",
    "#         'domain=social?':raw_vars_social_map.get(var, np.nan),\n",
    "#         'domain=cognitive?':raw_vars_cognitive_map.get(var, np.nan),\n",
    "#         'replicating know human bias?':raw_vars_known_map.get(var, np.nan),\n",
    "#         'preference measure?':raw_vars_pref_map.get(var, np.nan),\n",
    "#         'stimuli dependent?':raw_vars_stim_map.get(var, np.nan),\n",
    "#         'sample size':        n\n",
    "#     })\n",
    "\n",
    "for var in DV_vars:\n",
    "    col_h = f\"{var}_human\"\n",
    "    col_t = f\"{var}_twin\"\n",
    "    min_col = f\"{var}_min\"\n",
    "    max_col = f\"{var}_max\"\n",
    "    if cond_exists:\n",
    "        cols = [col_h, col_t, cond_h, cond_t, min_col, max_col]\n",
    "    else:\n",
    "        cols = [col_h, col_t, min_col, max_col]\n",
    "    pair = df[cols].dropna(subset=[col_h, col_t])\n",
    "    min_val = pair[min_col].iloc[0]\n",
    "    max_val = pair[max_col].iloc[0]\n",
    "    n = len(pair)\n",
    "    if n >= 4:\n",
    "        r, _ = pearsonr(pair[col_h], pair[col_t])\n",
    "        z_f = np.arctanh(r)\n",
    "        se = 1 / np.sqrt(n - 3)\n",
    "        z_crit = norm.ppf(0.975)\n",
    "        lo_z, hi_z = z_f - z_crit * se, z_f + z_crit * se\n",
    "        lo_r, hi_r = np.tanh(lo_z), np.tanh(hi_z)\n",
    "        z_score = z_f / se\n",
    "        # Accuracy = mean absolute diff / range\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "            accuracy = np.nan\n",
    "        else:\n",
    "            # compute mean absolute difference\n",
    "            abs_diff = np.abs(pair[col_h] - pair[col_t])\n",
    "            mean_abs_diff = abs_diff.mean()\n",
    "            accuracy = 1 - mean_abs_diff / (max_val - min_val)\n",
    "\n",
    "        mean_h = pair[col_h].mean()\n",
    "        mean_t = pair[col_t].mean()\n",
    "\n",
    "        # Paired t‐test\n",
    "        t_stat, p_val = ttest_rel(pair[col_h], pair[col_t])\n",
    "\n",
    "        std_h = pair[col_h].std(ddof=1)\n",
    "        std_t = pair[col_t].std(ddof=1)\n",
    "\n",
    "        # F‐test for equal variances\n",
    "        df1 = df2 = n - 1\n",
    "        f_stat = (std_h**2 / std_t**2) if std_t > 0 else np.nan\n",
    "        # two‐tailed p‐value:\n",
    "        if not np.isnan(f_stat):\n",
    "            p_f = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n",
    "        else:\n",
    "            p_f = np.nan\n",
    "\n",
    "        # Effect sizes (Cohen's d) across conditions\n",
    "        #    For humans:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_h = pair[cond_h].unique()\n",
    "            if len(levels_h) == 2:\n",
    "                g1 = pair.loc[pair[cond_h] == levels_h[0], col_h]\n",
    "                g2 = pair.loc[pair[cond_h] == levels_h[1], col_h]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                # pooled sd\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_human = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_human = np.nan\n",
    "        else:\n",
    "            d_human = np.nan\n",
    "\n",
    "        #    For twins:\n",
    "        if cond_exists and len(pair) > 3:\n",
    "            levels_t = pair[cond_t].unique()\n",
    "            if cond_exists and len(levels_t) == 2:\n",
    "                g1 = pair.loc[pair[cond_t] == levels_t[0], col_t]\n",
    "                g2 = pair.loc[pair[cond_t] == levels_t[1], col_t]\n",
    "                n1, n2 = len(g1), len(g2)\n",
    "                s_pool = np.sqrt(\n",
    "                    ((n1 - 1) * g1.var(ddof=1) + (n2 - 1) * g2.var(ddof=1)) / (n1 + n2 - 2)\n",
    "                )\n",
    "                d_twin = (g1.mean() - g2.mean()) / s_pool if s_pool > 0 else np.nan\n",
    "            else:\n",
    "                d_twin = np.nan\n",
    "        else:\n",
    "            d_twin = np.nan\n",
    "    else:\n",
    "        r = lo_r = hi_r = z_score = accuracy = mean_h = mean_t = t_stat = p_val = std_h = std_t = (\n",
    "            f_stat\n",
    "        ) = p_f = np.nan\n",
    "        d_human = d_twin = np.nan\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"study name\": study_name,\n",
    "            \"persona specification\": specification_name,\n",
    "            \"variable name\": var,\n",
    "            #        'variable type (raw response/DV)':     'DV',\n",
    "            \"correlation between the responses from humans vs. their twins\": r,\n",
    "            \"CI_lower\": lo_r,\n",
    "            \"CI_upper\": hi_r,\n",
    "            \"z-score for correlation between humans vs. their twins\": z_score,\n",
    "            \"accuracy between humans vs. their twins\": accuracy,\n",
    "            \"mean_human\": mean_h,\n",
    "            \"mean_twin\": mean_t,\n",
    "            \"paired t-test t-stat\": t_stat,\n",
    "            \"paired t-test p-value\": p_val,\n",
    "            \"std_human\": std_h,\n",
    "            \"std_twin\": std_t,\n",
    "            \"variance test F-stat\": f_stat,\n",
    "            \"variance test p-value\": p_f,\n",
    "            \"effect size based on human\": d_human,\n",
    "            \"effect size based on twin\": d_twin,\n",
    "            \"domain=social?\": DV_vars_social_map.get(var, np.nan),\n",
    "            \"domain=cognitive?\": DV_vars_cognitive_map.get(var, np.nan),\n",
    "            \"replicating know human bias?\": DV_vars_known_map.get(var, np.nan),\n",
    "            \"preference measure?\": DV_vars_pref_map.get(var, np.nan),\n",
    "            \"stimuli dependent?\": DV_vars_stim_map.get(var, np.nan),\n",
    "            \"knowledge question?\": DV_vars_know_map.get(var, np.nan),\n",
    "            \"political question?\": DV_vars_politics_map.get(var, np.nan),\n",
    "            \"sample size\": n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# results DataFrame\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(corr_df)\n",
    "\n",
    "# save output as csv - unit of observation is comparison between humans and twins:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis.csv\"\n",
    "corr_df.to_csv(out_file, index=False)\n",
    "\n",
    "#####participant-level data:\n",
    "\n",
    "\n",
    "def make_long(df, respondent_type):\n",
    "    # pick off TWIN_ID + the DVs, then melt\n",
    "    long = df[[\"TWIN_ID\"] + DV_vars].melt(\n",
    "        id_vars=\"TWIN_ID\", value_vars=DV_vars, var_name=\"variable_name\", value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    long[\"respondent_type\"] = respondent_type\n",
    "    long[\"study_name\"] = study_name\n",
    "    long[\"specification_name\"] = specification_name\n",
    "    return long\n",
    "\n",
    "\n",
    "#########################unique to this study:\n",
    "# turn the index back into a column so melt can see it\n",
    "df_human.reset_index(inplace=True)\n",
    "df_twin.reset_index(inplace=True)\n",
    "###################\n",
    "\n",
    "\n",
    "# build the two halves\n",
    "long_h = make_long(df_human, \"human\")\n",
    "long_t = make_long(df_twin, \"twin\")\n",
    "\n",
    "# stack them\n",
    "df_long = pd.concat([long_h, long_t], ignore_index=True)\n",
    "\n",
    "print(df_long.head())\n",
    "# save output as csv - unit of observation is TWIN_ID:\n",
    "out_file = f\"{study_name} {specification_name} meta analysis individual level.csv\"\n",
    "df_long.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce42d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
